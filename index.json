[{"authors":["admin"],"categories":null,"content":"I am a pulmonary and critical care physician at the University of Pennsylvania Perelman School of Medicine, and core faculty at the Palliative and Advanced Illness Research (PAIR) Center. My research interests include clinical informatics, natural language processing, machine learning, population health, and pragmatic trials. I am also a Senior Fellow at the Leonard Davis Institute of Health Economics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1556122809,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am a pulmonary and critical care physician at the University of Pennsylvania Perelman School of Medicine, and core faculty at the Palliative and Advanced Illness Research (PAIR) Center. My research interests include clinical informatics, natural language processing, machine learning, population health, and pragmatic trials. I am also a Senior Fellow at the Leonard Davis Institute of Health Economics.","tags":null,"title":"Gary Weissman","type":"author"},{"authors":null,"categories":["blog","r","bootstrapping","predictive modeling"],"content":" Background Bootstrapping is a helpful technique for identifying the variance of an estimate in a given sample when no other data are available. In the case of the evaluation of clinical prediction models, bootstrapping can be used to estimate confidence intervals around performance metrics such as the Brier score, the c-statistic, and others. When the original model and its tuning parameters, and the original dataset are available, the data can be resampled and the model refit on each replicate to make predictions. These predictions can then be used to calculate a performance metric. However, sometimes only the predicted probabilities of the original model on the original dataset are available. In this case, the predicted probabilities can be resampled and the performance metric can be calculated on each of these replicates. How do these approaches compare?\n Build a model Similar to the model built in another blog post about the Brier Score, let’s build a model with the abalone dataset.\n# set seed set.seed(24601) # load libraries for plotting library(ggplot2) # get data df \u0026lt;- read.csv(\u0026#39;https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\u0026#39;) names(df) \u0026lt;- c(\u0026#39;sex\u0026#39;, \u0026#39;length\u0026#39;, \u0026#39;diameter\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;weight_whole\u0026#39;, \u0026#39;weight_shucked\u0026#39;, \u0026#39;weight_viscera\u0026#39;, \u0026#39;weight_shell\u0026#39;, \u0026#39;rings\u0026#39;) # inspect data knitr::kable(head(df))   sex length diameter height weight_whole weight_shucked weight_viscera weight_shell rings    M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7  F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9  M 0.440 0.365 0.125 0.5160 0.2155 0.1140 0.155 10  I 0.330 0.255 0.080 0.2050 0.0895 0.0395 0.055 7  I 0.425 0.300 0.095 0.3515 0.1410 0.0775 0.120 8  F 0.530 0.415 0.150 0.7775 0.2370 0.1415 0.330 20    # Let\u0026#39;s predict whether or not an abalone will have \u0026gt; 10 rings m \u0026lt;- glm(I(rings \u0026gt; 10) ~ ., data = df, family = binomial) preds_m \u0026lt;- predict(m, type = \u0026#39;response\u0026#39;)  Performance metric Let’s calculate the Brier score as a measure of model performance.\nbrier_score \u0026lt;- function(preds, obs) { mean((obs - preds)^2) }  Boostrapping the predicted probabilities only Now let’s estimate the 95% confidence interval of the Brier score using only the predicted probabilities.\nN \u0026lt;- 1000 get_boot_est_preds \u0026lt;- function(preds, obs, metric) { idx \u0026lt;- sample(length(preds), replace = TRUE) metric(preds[idx], obs[idx]) } reps_pred \u0026lt;- replicate(N, get_boot_est_preds(preds_m, df$rings \u0026gt; 10, brier_score))  Bootstrapping to refit the model on each replicate get_boot_est_mod \u0026lt;- function(df, metric) { idx \u0026lt;- sample(nrow(df), replace = TRUE) m_b \u0026lt;- glm(I(rings \u0026gt; 10) ~ ., data = df[idx,], family = binomial) preds_m_b \u0026lt;- predict(m_b, type = \u0026#39;response\u0026#39;) metric(preds_m_b, df[idx,]$rings \u0026gt; 10) } reps_model \u0026lt;- replicate(N, get_boot_est_mod(df, brier_score))  Evaluate results Let’s look at the distrbution of the Brier score estimates using both approaches.\nres \u0026lt;- rbind(data.frame(brier_score = reps_pred, approach = \u0026#39;predictions\u0026#39;), data.frame(brier_score = reps_model, approach = \u0026#39;refit_model\u0026#39;)) # Plot distribution of bootstrapped Brier scores ggplot(res, aes(brier_score, color = approach)) + geom_density() + theme_bw() We can calculate 95% confidence intervals using a simple percentile approach.\ncalc_ci_95 \u0026lt;- function(v) { q \u0026lt;- format(quantile(v, probs = c(0.025, 0.975)), digits = 5) paste0(\u0026#39;(\u0026#39;,q[1],\u0026#39; to \u0026#39;,q[2],\u0026#39;)\u0026#39;) } cat(\u0026#39;CI using bootstrapped estimates from predictions only:\u0026#39;, calc_ci_95(reps_pred),\u0026#39;\\n\u0026#39;) ## CI using bootstrapped estimates from predictions only: (0.14201 to 0.15427) cat(\u0026#39;CI using bootstrapped estimates from refitting the model:\u0026#39;, calc_ci_95(reps_model),\u0026#39;\\n\u0026#39;) ## CI using bootstrapped estimates from refitting the model: (0.14106 to 0.15401)  Summary While not exactly the same, the two approaches yield very similar results. However, this outcome may vary depending on potential bias in the original dataset, and the sensitivity of the model to such bias.\n ","date":1556150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556202871,"objectID":"9b3e536ea78160bfe348fe66eeff3bc3","permalink":"/post/comparison-of-bootstrapping-approaches-for-identifying-variance-of-predictive-performance/","publishdate":"2019-04-25T00:00:00Z","relpermalink":"/post/comparison-of-bootstrapping-approaches-for-identifying-variance-of-predictive-performance/","section":"post","summary":"Background Bootstrapping is a helpful technique for identifying the variance of an estimate in a given sample when no other data are available. In the case of the evaluation of clinical prediction models, bootstrapping can be used to estimate confidence intervals around performance metrics such as the Brier score, the c-statistic, and others. When the original model and its tuning parameters, and the original dataset are available, the data can be resampled and the model refit on each replicate to make predictions.","tags":[],"title":"Comparison of bootstrapping approaches for identifying variance of predictive performance","type":"post"},{"authors":null,"categories":["brier score","predictive modeling"],"content":" Background The Brier Score is a composite measure of discrimination and calibration for a prediction model. The Brier Score is defined as\n\\[ BS = \\frac{1}{N} \\sum (y_i - \\hat y_i)^2, \\]\nwhere \\(N\\) is the number of observations, \\(y_i\\) is the observed outcome, either 0 or 1, and \\(\\hat y_i\\) is the predicted probability for the \\(i\\)th observation. Let’s create an R function calculate the Brier Score:\nbrier_score \u0026lt;- function(obs, pred) { mean((obs - pred)^2) } The scaled Brier Score accounts for the event rate and provides an immediate comparison to an uninformative model that is equivalent to “just guess the event rate.” An intuitive way to define the scaled Brier Score (also called the “Brier skill score”) is\n\\[ BS_{scaled} = 1 - \\frac{BS}{BS_{max}}, \\]\nwhere \\(BS_{max} = \\frac{1}{N} \\sum (y_i - \\bar y_i)^2\\) and \\(\\bar y_i\\) is the event rate among the observed outcome.\n My confusion This formulation of the scaled Brier Score makes intuitive sense to me and is how I go about calculating it in practice. However, two other distinct formulations have been proposed for calculating \\(BS_{max}\\) that — at least to the limits of my algebraic skills – differ. Thus, here I proposed a numeric investigation of these different definitions to see if they are indeed equivalent.\n Definition 1 This is the intuitive definition to which I am accustomed, and is made explicit here: https://www.ncbi.nlm.nih.gov/pubmed/29713202\n\\[ BS_{scaled} = 1 - \\frac{\\frac{1}{N} \\sum (y_i - \\hat y_i)^2}{\\frac{1}{N} \\sum (y_i - \\bar y_i)^2}. \\]\nLet’s create an R function to calculate this value.\nscaled_brier_score_1 \u0026lt;- function(obs, pred) { 1 - (brier_score(obs, pred) / brier_score(obs, mean(obs))) }  Definition 2 A second formulation of the scaled Brier Score is defined with a slightly different definition of \\(BS_{max}\\), which is in this case described in https://www.ncbi.nlm.nih.gov/pubmed/20010215\n\\[ BS_{max} = \\bar p \\times (1 - \\bar p). \\]\nLet’s create an R function to calculate this measure.\nscaled_brier_score_2 \u0026lt;- function(obs, pred) { 1 - (brier_score(obs, pred) / (mean(obs) * (1 - mean(obs)))) }  Definition 3 A third formulation of the scaled Brier Score is defined with a slightly different definition of \\(BS_{max}\\), which is in this case described in https://www.ncbi.nlm.nih.gov/pubmed/22961910\n\\[ BS_{max} = \\bar p \\times (1 - \\bar p)^2 + (1 - \\bar p) \\times \\bar p^2. \\]\nLet’s create an R function to calculate this measure.\nscaled_brier_score_3 \u0026lt;- function(obs, pred) { 1 - (brier_score(obs, pred) / (mean(obs) * (1 - mean(obs))^2 + (1 - mean(obs)) * mean(obs)^2)) }  Build a model Let’s build a sample model based on the UCI abalone data (https://archive.ics.uci.edu/ml/datasets/Abalone).\n# get data df \u0026lt;- read.csv(\u0026#39;https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\u0026#39;) names(df) \u0026lt;- c(\u0026#39;sex\u0026#39;, \u0026#39;length\u0026#39;, \u0026#39;diameter\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;weight_whole\u0026#39;, \u0026#39;weight_shucked\u0026#39;, \u0026#39;weight_viscera\u0026#39;, \u0026#39;weight_shell\u0026#39;, \u0026#39;rings\u0026#39;) # inspect data knitr::kable(head(df))   sex length diameter height weight_whole weight_shucked weight_viscera weight_shell rings    M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7  F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9  M 0.440 0.365 0.125 0.5160 0.2155 0.1140 0.155 10  I 0.330 0.255 0.080 0.2050 0.0895 0.0395 0.055 7  I 0.425 0.300 0.095 0.3515 0.1410 0.0775 0.120 8  F 0.530 0.415 0.150 0.7775 0.2370 0.1415 0.330 20    # Let\u0026#39;s predict whether or not an abalone will have \u0026gt; 10 rings m1 \u0026lt;- glm(I(rings \u0026gt; 10) ~ ., data = df, family = binomial) preds_m1 \u0026lt;- predict(m1, type = \u0026#39;response\u0026#39;) # And another model with severe class imablance m2 \u0026lt;- glm(I(rings \u0026gt; 3) ~ ., data = df, family = binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred preds_m2 \u0026lt;- predict(m2, type = \u0026#39;response\u0026#39;)  Score the model # ---------- Model 1 # Calculate the Brier Score brier_score(df$rings \u0026gt; 10, preds_m1) ## [1] 0.1479862 # Calculate each type of scaled Brier Score scaled_brier_score_1(df$rings \u0026gt; 10, preds_m1) ## [1] 0.3462507 scaled_brier_score_2(df$rings \u0026gt; 10, preds_m1) ## [1] 0.3462507 scaled_brier_score_3(df$rings \u0026gt; 10, preds_m1) ## [1] 0.3462507 # ---------- Model 2 # Calculate the Brier Score brier_score(df$rings \u0026gt; 3, preds_m2) ## [1] 0.002690905 # Calculate each type of scaled Brier Score scaled_brier_score_1(df$rings \u0026gt; 3, preds_m2) ## [1] 0.3362851 scaled_brier_score_2(df$rings \u0026gt; 3, preds_m2) ## [1] 0.3362851 scaled_brier_score_3(df$rings \u0026gt; 3, preds_m2) ## [1] 0.3362851  ","date":1555977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556205798,"objectID":"84a76f3c896e4797207fe1ff0ec5b2d3","permalink":"/post/evaluating-the-equivalence-of-different-formulations-of-the-scaled-brier-score/","publishdate":"2019-04-23T00:00:00Z","relpermalink":"/post/evaluating-the-equivalence-of-different-formulations-of-the-scaled-brier-score/","section":"post","summary":"Background The Brier Score is a composite measure of discrimination and calibration for a prediction model. The Brier Score is defined as\n\\[ BS = \\frac{1}{N} \\sum (y_i - \\hat y_i)^2, \\]\nwhere \\(N\\) is the number of observations, \\(y_i\\) is the observed outcome, either 0 or 1, and \\(\\hat y_i\\) is the predicted probability for the \\(i\\)th observation. Let’s create an R function calculate the Brier Score:\nbrier_score \u0026lt;- function(obs, pred) { mean((obs - pred)^2) } The scaled Brier Score accounts for the event rate and provides an immediate comparison to an uninformative model that is equivalent to “just guess the event rate.","tags":[],"title":"Evaluating the equivalence of different formulations of the scaled Brier score","type":"post"},{"authors":["Gary Weissman","Lyle Ungar","Michael Harhay","Katherine Courtright","Scott Halpern"],"categories":null,"content":" Supplementary notes can be added here, including code and math.   -- ","date":1554091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554091200,"objectID":"c4f8bdb6f9495804b8a838d4d2b84d4a","permalink":"/publication/sentiment_2019/","publishdate":"2019-04-01T00:00:00-04:00","relpermalink":"/publication/sentiment_2019/","section":"publication","summary":"Assessment of six commonly available sentiment methods trained in non-clinical domains. We evaluated their performance at predicting mortality, and other clinical tasks. Existing sentiment methods have serious limitations with clinical data and should not be deployed to support decision making tools.","tags":null,"title":"Construct validity of six sentiment analysis methods in the text of encounter notes of patients with critical illness","type":"publication"},{"authors":["Gary Weissman"],"categories":null,"content":" Supplementary notes can be added here, including code and math.   -- ","date":1554091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554091200,"objectID":"99eaff80018a5f2f1dd26e3737a4b5fb","permalink":"/publication/hccs_2019/","publishdate":"2019-04-01T00:00:00-04:00","relpermalink":"/publication/hccs_2019/","section":"publication","summary":"Pulmonary diseases and various forms of critical illness play a significant role in this risk adjustment process both through their associated HCC codes and through interactions with other risk categories representing cardiac and psychiatric diseases.","tags":null,"title":"Hierarchical Condition Categories for Pulmonary Diseases: Population Health Management and Policy Opportunities.","type":"publication"},{"authors":null,"categories":["keras","rnn","python"],"content":"Recurrent neural networks and their variants are helpful for extracting information from time series. Here\u0026rsquo;s an example using sample data to get up and running.\nI found the following other websites helpful in reading up on code examples:\n https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/ https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_benchmark.py https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction/blob/master/lstm.py  # setup import numpy as np import pandas as pd import math import matplotlib.pyplot as plt from keras.models import Sequential from keras.layers import Dense, Dropout, SimpleRNN from keras.callbacks import EarlyStopping from sklearn.model_selection import train_test_split # make a sample multivariable time series - not autoregressive # generate a random walk def random_walk(steps, scale = 1): w = np.zeros(steps) for x in range(1,steps): w[x] = w[x-1] + scale * np.random.normal() return(w) time_steps = 5000 data = pd.DataFrame({'x' : range(time_steps), 'y' : np.arange(time_steps) ** (1/2) + random_walk(time_steps) }) data = data.assign(z = np.log(data.x+1) + 0.3 * data.y) data_mat = np.array(data)  Take a look at the data.\nplt.subplot(2,1,1) plt.plot(data_mat[:,0], data_mat[:,2], c = 'goldenrod') plt.margins(0.05) plt.subplot(2,1,2) plt.plot(data_mat[:,1], data_mat[:,2], c = 'firebrick') plt.margins(0.05) plt.show()  # split into samples (sliding time windows) samples = list() target = list() length = 50 # step over the 5,000 in jumps of length for i in range(time_steps - length - 1): # grab from i to i + length sample = data_mat[i:i+length,:2] outcome = data_mat[i+length+1,2] target.append(outcome) samples.append(sample) # split out a test set test_size = 1000 x_test_mat = np.dstack(samples[-test_size:]) x_test_3d_final = np.moveaxis(x_test_mat, [0, 1, 2], [1, 2, 0] ) # The RNN needs data with the format of [samples, time steps, features]. # Here, we have N samples, 50 time steps per sample, and 2 features data_mat_stacked = np.dstack(samples[:-test_size]) data_mat_3d_final = np.moveaxis(data_mat_stacked, [0, 1, 2], [1, 2, 0] ) # and fix up the target target_arr = np.array(target[:-test_size]) # now build the RNN model = Sequential() model.add(SimpleRNN(128, input_shape = (data_mat_3d_final.shape[1], data_mat_3d_final.shape[2]), activation = 'relu')) model.add(Dropout(0.1)) model.add(Dense(64, activation = 'relu')) model.add(Dropout(0.1)) model.add(Dense(16, activation = 'relu')) model.add(Dropout(0.1)) model.add(Dense(1, activation='linear')) # monitor validation progress early = EarlyStopping(monitor = \u0026quot;val_loss\u0026quot;, mode = \u0026quot;min\u0026quot;, patience = 7) callbacks_list = [early] model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mse']) # and train the model history = model.fit(data_mat_3d_final, target_arr, epochs=50, batch_size=25, verbose=2, validation_split = 0.20, callbacks = callbacks_list) # plot history plt.plot(history.history['loss'], label='train') plt.plot(history.history['val_loss'], label='val') plt.legend() plt.show()  # get predictions train_predictions = model.predict(data_mat_3d_final) test_predictions = model.predict(x_test_3d_final) # plot predictions vs actual plt.plot(data['x'], data['z'], c = 'goldenrod', label = 'data') plt.plot(data.iloc[(length+1):-test_size]['x'], train_predictions, c = 'navy', label = 'train') plt.plot(data.iloc[-test_size:]['x'], test_predictions, c = 'firebrick', label = 'test') plt.legend(loc='best') plt.show()  Not bad!\n","date":1518652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556068367,"objectID":"3b658f449f2e66d6f77fa54c9979fa28","permalink":"/post/building-a-recurrent-neural-network-to-predict-time-series-data-with-keras-in-python/","publishdate":"2018-02-15T00:00:00Z","relpermalink":"/post/building-a-recurrent-neural-network-to-predict-time-series-data-with-keras-in-python/","section":"post","summary":"Recurrent neural networks and their variants are helpful for extracting information from time series. Here\u0026rsquo;s an example using sample data to get up and running.\nI found the following other websites helpful in reading up on code examples:\n https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/ https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_benchmark.py https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction/blob/master/lstm.py  # setup import numpy as np import pandas as pd import math import matplotlib.pyplot as plt from keras.models import Sequential from keras.layers import Dense, Dropout, SimpleRNN from keras.","tags":[],"title":"Building a recurrent neural network to predict time-series data with Keras in Python","type":"post"},{"authors":null,"categories":["R","maps"],"content":" Every now and then it is useful to make a map. In times of political uncertainty, data can light a path forward. Your local elected officals may be interested in data, too, and how they impact policy (and re-election, of course).\nLet\u0026rsquo;s say you wanted to know if people are still enrolling the Healthcare.gov plans in your local State Senatorial district. It\u0026rsquo;s quite difficult to find data broken up at this level but zip codes can approximate such an approach. Here\u0026rsquo;s information from Pennsylvania 26th Senatorial District.\nMarketplace enrollment Using publicly available data from the Centers for Medicare \u0026amp; Medicaid Services, this file includes data by zip code on enrollment through HealthCare.gov The data description reads:\n\u0026ldquo;2017 OEP ZIP Code-Level Public Use File: This ZIP code and APTC PUF includes total health plan selections, the count of consumers with APTC, and average APTC among consumers with APTC between November 1, 2016 and January 31, 2017. This PUF only includes data for the 39 states that used the HealthCare.gov platform in 2017.\u0026rdquo;\n# setup require(zipcode) library(readxl) require(dplyr) require(choroplethrZip) require(magrittr) data(zipcode) # download and extract this file: https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Marketplace-Products/Plan_Selection_ZIP.html dt \u0026lt;- read_excel('2017_OEP_ZIP_Code-Level_Public_Use_File.xlsx', sheet = 4) # approximating the 26th district zips_26thsenate \u0026lt;- c('19029', '19081', '19082', '19078', '19036', '19033', '19070', '19018', '19026', '19064', '19063', '19008', '19073', '19355', '19301', '19050') dt %\u0026lt;\u0026gt;% select(ZipCode = 'ZIP Code', Enrollees = 'Total Number of Consumers Who Have Selected a Marketplace Plan') %\u0026gt;% mutate(Enrollees = as.integer(Enrollees)) %\u0026gt;% filter(ZipCode %in% zips_26thsenate) %\u0026gt;% left_join(zipcode, by = c('ZipCode' = 'zip'))  Summarize the results by zip:\ndt %\u0026gt;% select(ZipCode, City = city, Enrollees) %\u0026gt;% arrange(desc(Enrollees)) %\u0026gt;% knitr::kable()  And plot the output:\nzipmap \u0026lt;- dt %\u0026gt;% select(value = Enrollees, region = ZipCode) %\u0026gt;% zip_choropleth(zip_zoom = zips_26thsenate, reference_map = TRUE, title = 'Healthcare.gov enrollees November 1, 2016 - January 21, 2017') plot(zipmap)  ","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556068521,"objectID":"e4360c82309ef5cfe0cc264456f46907","permalink":"/post/making-choropleth-maps-by-zip-code/","publishdate":"2017-07-17T00:00:00Z","relpermalink":"/post/making-choropleth-maps-by-zip-code/","section":"post","summary":"Every now and then it is useful to make a map. In times of political uncertainty, data can light a path forward. Your local elected officals may be interested in data, too, and how they impact policy (and re-election, of course).\nLet\u0026rsquo;s say you wanted to know if people are still enrolling the Healthcare.gov plans in your local State Senatorial district. It\u0026rsquo;s quite difficult to find data broken up at this level but zip codes can approximate such an approach.","tags":[],"title":"Making choropleth maps by zip code","type":"post"},{"authors":null,"categories":["babelgraph","blog","social network analysis"],"content":"Here\u0026rsquo;s the blog post originally posted on babelgraph.org on March 25, 2011. The relevant hyperlinks and igraph code have been updated to reflect the current state of things. The edgelist, despite many more intervening seasons, and thus many more romances, has not been updated. Here is a nice walkthrough of ERGMs based on this post \u0026ndash; sorry for the broken link, Ben.\nThis all began with an introductory presentation about social network analysis to a group of medical students. What better way to grab their attention than with attractive, fake doctors having sex on television? Naturally this led to the dense network of sexual contacts between characters on the Grey’s Anatomy television show. After viewing many hours of previous episodes and exploring fan pages (especially here for an early attempt at a graph representation of sexual contacts), I was able to come up with an extensive but by no means exhaustive list of contacts. The edge list is available here.\nThis example uses the igraph package for R, both free to download. First we create the graph, give it a layout, and plot.\nlibrary(igraph) ga.data \u0026lt;- read.csv('ga_edgelist.csv', header = TRUE) g \u0026lt;- graph.data.frame(ga.data, directed=FALSE) summary(g) g$layout \u0026lt;- layout.fruchterman.reingold(g) plot(g)  Without knowing who is represented by each vertex, what can you deduce from the graph? From a public health perspective, if you could test one person for sexually transmitted infections (STIs), who would it be? If you could provide counseling and a free box of condoms to one person, who would it be? If you knew that an epidemic was spreading through this network, who would you want to be to best avoid it?\nNow let’s make the visualization a little more interesting. First we can remove the labels for now, and then change the size of the vertex to represent the degree, or degree centrality, corresponding to the number of partners of each vertex. In the context of transmissible infections, this would indicate the number of people a person could infect or be infected by through sexual contact.\nV(g)$label \u0026lt;- NA # remove labels for now V(g)$size \u0026lt;- degree(g) * 2 # multiply by 2 for scale plot(g)  This tells us about the absolute number of partners, but not much about the relative position within the network. Let’s examine two types of centrality: closeness and betweenness. The closeness centrality is the average shortest path from one vertex to every other on the graph. A high number indicates that a vertex is quickly reachable by the majority of vertices in the graph, while a low number indicates that the vertex is far from most other vertices on the graph. We can calculate the centrality and then rescale the values to create a color scheme to visualize the relative differences.\nclo \u0026lt;- closeness(g) # rescale values to match the elements of a color vector clo.score \u0026lt;- round( (clo - min(clo)) * length(clo) / max(clo) ) + 1 # create color vector, use rev to make red \u0026quot;hot\u0026quot; clo.colors \u0026lt;- rev(heat.colors(max(clo.score))) V(g)$color \u0026lt;- clo.colors[ clo.score ] plot(g)  It appears there are a few vertices on the red “hot” end of the spectrum, and a few at the “cold” end. Next we do the same for each vertex to calculate the betweenness centrality, which is the number of shortest paths on the network that pass through the vertex. Vertices with high betweenness centrality might be thought of as serving a gatekeeper role in mediating the shortest connections between other vertices.\nbtw \u0026lt;- betweenness(g) btw.score \u0026lt;- round(btw) + 1 btw.colors \u0026lt;- rev(heat.colors(max(btw.score))) V(g)$color \u0026lt;- btw.colors[ btw.score ] plot(g)  This last graph of betweenness indicates slightly more variation among the likely suspects, while the analysis of closeness centrality demonstrated less variation. Why?\nA useful technique in social network analysis is the use of community finding algorithms. Here we use the implementation of the Girvan-Newman algorithm (paper here) to detect the underlying community structure of the graph. We will iterate through each merge to determine which cut produces the maximum modularity, and then use that number to calculate the groups.\n# this section has been substantially revised to reflect the # newer version of igraph which does GN membership easily V(g)$color \u0026lt;- membership(cluster_edge_betweenness(g)) V(g)$size \u0026lt;- 15 # reset to default size plot(g) # those new default igraph colors are so much nicer, too!  Once you see the graph with names, it is interesting to note the breaks in connectivity around race and age (I guess you have to know the TV characters to appreciate this ;) ) So before seeing the names, back to the original question. Who would you test? Who would you counsel? Who would you vaccinate? Who would you rather be?\nAnd the winners are…\nV(g)$color \u0026lt;- 'grey' V(g)$label \u0026lt;- V(g)$name V(g)$label.cex \u0026lt;- 0.7 # rescale the text size of the label plot(g)  ","date":1497484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556067994,"objectID":"37fc9011f34f7a12f94f2f3bbeac8aeb","permalink":"/post/grey-s-anatomy-network-of-sexual-relations/","publishdate":"2017-06-15T00:00:00Z","relpermalink":"/post/grey-s-anatomy-network-of-sexual-relations/","section":"post","summary":"Here\u0026rsquo;s the blog post originally posted on babelgraph.org on March 25, 2011. The relevant hyperlinks and igraph code have been updated to reflect the current state of things. The edgelist, despite many more intervening seasons, and thus many more romances, has not been updated. Here is a nice walkthrough of ERGMs based on this post \u0026ndash; sorry for the broken link, Ben.\nThis all began with an introductory presentation about social network analysis to a group of medical students.","tags":[],"title":"Grey’s Anatomy Network of Sexual Relations","type":"post"},{"authors":null,"categories":["twitter","r","babelgraph","blog"],"content":"Here\u0026rsquo;s the blog post originally posted on babelgraph.org on April 4, 2011. I suspect the twitter API and much of the code below are not up to date.\nI am a recent comer to twitter, and it took me a few weeks to figure out what this was all about. Who are all these people tweeting each other and what do all these trending hashtags mean? Do these people all know each other? How do they get involved in these conversations? Are people really talking/listening to each other, or just spewing 140 character projectiles out into the void?\nThis piqued my interest in the structure of relationships of participants in different twitter conversations. Using R, with the twitteR and igraph packages, I wondered what I would find\u0026hellip;\nIn terms of mapping twitter networks with R, I found this post for mapping one’s own personal network of connections. For other visualizations, I found a collection of various visualizations for the twitterverse, and an interactive map of connected users.\nHere I am really interested in the network of participants in a particular conversation. I started off with “#Rstats,” expecting to see a tightly knit community of users who all seem to know each other (at least virtually), sending each other personal updates or tech support tweets. Next I looked at “#Libya,” where I imagined there would be a lot of twitterers with massive followings, but who were unlikely following each other, and producing hordes of news-type updates. Finally I looked at “#Bieber,” (I swear didn’t even know who he was up until a few weeks ago), where I expected to see platoons of small-time twitterers, who likely don’t follow each other, pouring out disconnected digital adulations into the twitterverse.\nThe following script was used to construct the networks for each search item.\n# an R script to generate the social network of contributors # to a particular search term # author: Gary Weissman # twitter: @garyweissman # choose a conversation and size of search search.term \u0026lt;- '#Rstats' search.size \u0026lt;- 50 # load required libraries library(igraph) library(twitteR) # get recent results from search search.results \u0026lt;- searchTwitter(search.term,n=search.size) # build the vertex list vl \u0026lt;- vector() for (twt in search.results) vl \u0026lt;- c(vl,screenName(twt)) # calculate contribution of each user in the conversation vl \u0026lt;- as.data.frame(table(vl)) # provide nice names for data colnames(vl) \u0026lt;- c('user','tweets') # build the network of relations between contributors g \u0026lt;- graph.empty(directed=TRUE) g \u0026lt;- add.vertices(g,nrow(vl),name=as.character(vl$user), tweets=vl$tweets) V(g)$followers \u0026lt;- 0 # default to zero # count total number of followers in the larger twitterverse and # add relationships based on who follows whom within the conversation for (usr in V(g)) { # get the user info by name tuser \u0026lt;- getUser(V(g)$name[usr+1]) print(paste(\u0026quot;Getting info on\u0026quot;,screenName(tuser))) # count total followers in larger twitterverse V(g)$followers[usr+1] \u0026lt;- followersCount(tuser) # access as many followers as we can to see if any # appeared in the search results followers.list \u0026lt;- userFollowers(tuser,n=1200) for (tflwr in followers.list) { if (screenName(tflwr) %in% V(g)$name) g \u0026lt;- add.edges(g,c(as.vector(V(g)[ name == screenName(tflwr) ]),usr)) } print('Sleeping 10 min...') Sys.sleep(600); # don't exceed request limit } # layout the graph g$layout \u0026lt;- layout.fruchterman.reingold(g) # adjust size based on number of total followers # scale accordingly to keep readable V(g)$size = log( V(g)$followers ) * 1.8 # set label name and size V(g)$label=V(g)$name V(g)$label.cex = 0.6 # do something with color... tcolors \u0026lt;- rev(heat.colors(max(V(g)$tweets))) V(g)$color \u0026lt;- tcolors[ V(g)$tweets ] # make edge arrows less intrusive E(g)$arrow.size \u0026lt;- 0.3 # make symmetric connections a little easier to read E(g)$curved \u0026lt;- FALSE # fun to play with E(g)$color \u0026lt;- 'blue' # now plot... plot(g)  Technical notes: It wasn’t always clear to me what counted as a “request” in the twitter API, because even a small handful of requests with long results would sometimes seem to count as more toward the hourly quota. Putting a ten minute delay between requests with Sys.sleep(600) was enough to solve the problem. I also tweaked the layouts a bit using tkplot.getcoords(). NB. tkplot() does not permit certain graph attributes such as label.vertex.cex, so you have to remove it before opening the graph in tkplot. You can then just add it back later. More details on this tkplot issue here.\nThe size of the vertex is proportional to the log of the number of followers in the wider twitterverse, and the color of vertex represents the number of tweets found in the search results by that user. See code above for specifics.\n#Bieber\n#Libya\n#Rstats\nThe networks obviously look very different. Here is a plot of the degree distribution of each graph:\nEven more interesting is the simple comparison of the number of edges in each graph. Very telling.\nI did not look at the clustering coefficient here, but that would be interesting, too.\nDoes this mean that R users are part of a twitter community?\nLimitations on this approach include the size of the user sample (limited by the twitter API to 150 requests per hour), as well as limits on the size of lists retrieved by the userFriends() method (I believe the limit is 1200 items). These former limits the sample size significantly, and provides only a brief snapshot in time of the participants in any one conversation. The latter favors the construction of edges between twitterers with fewer followers, thus understestimating the total edges that may exist in the network. Is there a workaround for these limitations on the API? I believe requesting whitelist status from twitter, or perhaps authenticating with ROAuth would increase the number of permitted requests, but I haven’t tried them yet.\nDo you know to whom you tweet?\n","date":1497484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556123149,"objectID":"e73486bd40f80a4c177c37a1ecf87fb8","permalink":"/post/the-structure-of-twitter-participant-relationships-in-conversations-around-libya-bieber-and-rstats/","publishdate":"2017-06-15T00:00:00Z","relpermalink":"/post/the-structure-of-twitter-participant-relationships-in-conversations-around-libya-bieber-and-rstats/","section":"post","summary":"Here\u0026rsquo;s the blog post originally posted on babelgraph.org on April 4, 2011. I suspect the twitter API and much of the code below are not up to date.\nI am a recent comer to twitter, and it took me a few weeks to figure out what this was all about. Who are all these people tweeting each other and what do all these trending hashtags mean? Do these people all know each other?","tags":[],"title":"The structure of twitter participant relationships in conversations around #Libya, #Bieber, and #Rstats","type":"post"},{"authors":null,"categories":["r","maps"],"content":" Not infrequently I am in a research meeting and someone says, \u0026ldquo;It would be really cool to get data on travel times for people. But I don\u0026rsquo;t know where to find that.\u0026rdquo; Here you will find just that. Please note that Google Maps terms of service are relevant for large data requests and potential privacy concerns for protected health information.\nCase Mr. Jones spends all his time at a coffee shop. Mr. Jones has hypertension, diabetes, and hyperlipidemia, and it doesn\u0026rsquo;t seem likely he will visit his primary care doctor anytime soon. Can we describe his potential travel time between his favorite coffee shop and his primary care doctor to better understand his transportation barriers? Why yes, yes we can.\nTransport library(ggmap) # quick start to ggmap: https://github.com/dkahle/ggmap # Specify where Mr. Jones is and where he's going # you can also use specific addresses if you already have them origin \u0026lt;- 'Hobbs Coffee Shop, Swarthmore' destination \u0026lt;- 'Penn Center for Primary Care, Philadelphia' # get some routes -- driving my_commute_drive \u0026lt;- route(origin, destination, structure = 'route', mode = 'driving') print(my_commute_drive)   m km miles seconds minutes hours leg lon lat 1 30 0.030 0.0186420 8 0.1333333 0.002222222 1 -75.35005 39.90210 2 162 0.162 0.1006668 56 0.9333333 0.015555556 2 -75.35033 39.90226 3 1047 1.047 0.6506058 123 2.0500000 0.034166667 3 -75.35089 39.90116 4 1829 1.829 1.1365406 214 3.5666667 0.059444444 4 -75.35296 39.89203 5 747 0.747 0.4641858 63 1.0500000 0.017500000 5 -75.34469 39.87802 6 4516 4.516 2.8062424 190 3.1666667 0.052777778 6 -75.35102 39.87342 7 5664 5.664 3.5196096 189 3.1500000 0.052500000 7 -75.30839 39.86862 8 5534 5.534 3.4388276 247 4.1166667 0.068611111 8 -75.24609 39.88126 9 2236 2.236 1.3894504 126 2.1000000 0.035000000 9 -75.19435 39.90546 10 1111 1.111 0.6903754 49 0.8166667 0.013611111 10 -75.19280 39.92460 11 347 0.347 0.2156258 20 0.3333333 0.005555556 11 -75.20004 39.93279 12 907 0.907 0.5636098 145 2.4166667 0.040277778 12 -75.19939 39.93587 13 1556 1.556 0.9668984 344 5.7333333 0.095555556 13 -75.19639 39.94367 14 141 0.141 0.0876174 31 0.5166667 0.008611111 14 -75.19798 39.95667 15 114 0.114 0.0708396 30 0.5000000 0.008333333 15 -75.19761 39.95791 16 105 0.105 0.0652470 41 0.6833333 0.011388889 16 -75.19893 39.95809 17 NA NA NA NA NA NA NA -75.19930 39.95843  Or try public transit.\n# route -- transit my_commute_transit \u0026lt;- route(origin, destination, structure = 'route', mode = 'transit') print(my_commute_transit)   m km miles seconds minutes hours leg lon lat 1 49 0.049 0.0304486 39 0.650000 0.01083333 1 -75.35005 39.90210 2 16750 16.750 10.4084500 1560 26.000000 0.43333333 2 -75.35083 39.90222 3 239 0.239 0.1485146 239 3.983333 0.06638889 3 -75.18166 39.95667 4 1624 1.624 1.0091536 180 3.000000 0.05000000 4 -75.18325 39.95489 5 424 0.424 0.2634736 313 5.216667 0.08694444 5 -75.20197 39.95719 6 NA NA NA NA NA NA NA -75.19930 39.95843  Let\u0026rsquo;s see what a walk would look like.\n# route -- walking my_commute_walk \u0026lt;- route(origin, destination, structure = 'route', mode = 'walking') print(my_commute_walk) # now plot the commute path qmap(destination, zoom = 11) + geom_path(aes(x=lon, y = lat), color = 'red', size = 1.5, data = my_commute_walk, lineend = 'round')  Mr. Jones, inspired by this map, just put down his triple-shot latte and is walking to his doctor\u0026rsquo;s office right now.\n","date":1497484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556122955,"objectID":"bd5f22217df7cf9c7d3c337aeaf1e435","permalink":"/post/transportation-data-in-r-why-yes-yes-you-can/","publishdate":"2017-06-15T00:00:00Z","relpermalink":"/post/transportation-data-in-r-why-yes-yes-you-can/","section":"post","summary":"Not infrequently I am in a research meeting and someone says, \u0026ldquo;It would be really cool to get data on travel times for people. But I don\u0026rsquo;t know where to find that.\u0026rdquo; Here you will find just that. Please note that Google Maps terms of service are relevant for large data requests and potential privacy concerns for protected health information.\nCase Mr. Jones spends all his time at a coffee shop.","tags":[],"title":"Transportation data in R: Why yes, yes you can","type":"post"},{"authors":null,"categories":["babelgraph","blog","meta"],"content":"Welcome to this blog, which is the new home for thoughts and technical things. As babelgraph.org has been retired, you can still get the code at:\nBabelGraph Alpha\nOver time I will try to migrate the old blog posts (mostly dealing with network analyses and C++ using R) to here along with the very informative user comments, and updates.\nFor old time\u0026rsquo;s sake, here is the old thumbnail gallery from the BabelGraph Alpha release (mixed Linux and Mac screenshots). A beautiful sight!\n","date":1497484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556122809,"objectID":"1c57548da8a1a34ec04e52bb69accdb6","permalink":"/post/welcome-to-the-blog/","publishdate":"2017-06-15T00:00:00Z","relpermalink":"/post/welcome-to-the-blog/","section":"post","summary":"Welcome to this blog, which is the new home for thoughts and technical things. As babelgraph.org has been retired, you can still get the code at:\nBabelGraph Alpha\nOver time I will try to migrate the old blog posts (mostly dealing with network analyses and C++ using R) to here along with the very informative user comments, and updates.\nFor old time\u0026rsquo;s sake, here is the old thumbnail gallery from the BabelGraph Alpha release (mixed Linux and Mac screenshots).","tags":[],"title":"Welcome to the blog!","type":"post"},{"authors":null,"categories":["babelgraph","blog","r"],"content":" Here\u0026rsquo;s the blog post originally posted on babelgraph.org on April 14, 2012.\nIn a recent blog post by CMastication, a little meme puzzle is presented with the introduction that a preschooler could solve it in 5-10 minutes, a programmer in an hour. I took the bait.\nThe original problem goes like this:\n8809=6 7111=0 2172=0 6666=4 1111=0 3213=0 7662=2 9313=1 0000=4 2222=0 3333=0 5555=0 8193=3 8096=5 7777=0 9999=4 7756=1 6855=3 9881=5 5531=0 2581=?  N.B. It turns out my strategy is completey wrong, but read on for an experiment with using eval and parse to generate code on the fly. An explanation and computational solution are available at the original site mentioned above.\nStrategy That being said, my first guess was to look for some set of operators +,-,*,/ between each of the digits that would produce the correct response. For example, 9313=1 could be calculated as 9 / 3 + 1 – 3 = 1, giving a solution of /+-. In order to try every combination of the four basic arithmetic operators in three different positions, I had to generate some R code on the fly.\nR writes R One of the things I like about R is that not only can I write R code, but R can write R code, too. We can create a line of code and store it as a variable. Then R will evaluate it whenever we like.\n{% gist gweissman/2377829 %}\nI can create the code dynamically by making a string representation of the code, then parsing it into an expression that can be evaluated as above.\n{% gist /gweissman/2385949 %}\nClearly it doesn’t give us the right answer.\nBut it was fun to code in R with R.\n","date":1465948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556123535,"objectID":"a91ee565905eb88bdcee9adf9fabb43e","permalink":"/post/r-can-write-r-code-too/","publishdate":"2016-06-15T00:00:00Z","relpermalink":"/post/r-can-write-r-code-too/","section":"post","summary":"Here\u0026rsquo;s the blog post originally posted on babelgraph.org on April 14, 2012.\nIn a recent blog post by CMastication, a little meme puzzle is presented with the introduction that a preschooler could solve it in 5-10 minutes, a programmer in an hour. I took the bait.\nThe original problem goes like this:\n8809=6 7111=0 2172=0 6666=4 1111=0 3213=0 7662=2 9313=1 0000=4 2222=0 3333=0 5555=0 8193=3 8096=5 7777=0 9999=4 7756=1 6855=3 9881=5 5531=0 2581=?","tags":[],"title":"R can write R code, too","type":"post"},{"authors":null,"categories":["babelgraph","blog","r","rcpp"],"content":" Here\u0026rsquo;s the blog post originally posted on babelgraph.org on July 11, 2012. Thanks to Hadley Wickham for referencing some of content here, and apologies for the broken URL. NB. The original C++ code didn\u0026rsquo;t seem to compile on my computer today. It required a small tweak with NumericVector::create \u0026ndash; see below.\nIn particular, R data frames provide a simple framework for representing large cohorts of agents in stochastic epidemiological models, such as those representing disease transmission. This approach is much easier and likely faster than trying to implement cohorts of R objects. In this post we’ll explore a simple agent-based model, and then benchmark a few different approaches to iterating through the cohort. Rcpp outperforms all of them by a few orders of magnitude. Priceless.\nCase Let’s say we are trying to predict the probability of someone choosing to receive a vaccination in a given year. The decision will be based on their age (age), gender (female), and whether or not they were infected with the virus last year (ily). Let’s make up some data:\n# construct a cohort N \u0026lt;- 1000 # cohort size cohort \u0026lt;- data.frame(age=rnorm(N,mean=50,sd=10), female=sample(c(0,1),size=N,replace=TRUE), ily=sample(c(0,1),size=N,prob=c(0.8,0.2), replace=TRUE))  The probability of choosing to be vaccinated will be given by the following function:\nvaccinate \u0026lt;- function( age, female, ily) { # this is based on some pretend regression equation p \u0026lt;- 0.25 + 0.3 * 1/(1-exp(0.04 * age)) + 0.1 *ily # use some logic p \u0026lt;- p * ifelse(female, 1.25 , 0.75) # boundary checking p \u0026lt;- max(0,p); p \u0026lt;- min(1,p) p }  Try some iterators: for loop, apply, ddply Let’s create a testable function for each strategy. The objective is to take a cohort data frame as input, calculate the vaccination probability for each member of the cohort, then return a data frame with the cohort data plus a new column for the vaccination probability (p).\n# The traditional for loop # Some would say is always a no-no in R... do_forloop \u0026lt;- function(df) { v_prob \u0026lt;- vector(length=nrow(df),mode=\u0026quot;numeric\u0026quot;) for (x in 1:nrow(df)) { v_prob[x] \u0026lt;- vaccinate(df$age[x], df$female[x],df$ily[x]) } data.frame(cbind(df,p=v_prob)) } # The apply approach do_apply \u0026lt;- function(df) { v_prob \u0026lt;- apply(df, 1, function(x) vaccinate(x[1],x[2],x[3])) data.frame(cbind(df,p=v_prob)) } # ddply approach library(plyr) do_plyr \u0026lt;- function (df) { v_prob \u0026lt;- ddply (df, names(df), function(x) vaccinate(x$age,x$female,x$ily)) data.frame(cbind(df,p=v_prob$V1)) }  Enter Rcpp Now rewrite the test using a traditional for-loop in C++ including a helper function to calculate the vaccination probability. I use the inline library so I can embed the C++ directly in the R script, thus obviating additional .cpp or .h files. Self-contained code is nice.\n# create an R function built on C++ code library(Rcpp) # required for inline Rcpp calls library(inline) # write the C++ code do_rcpp_src \u0026lt;- ' // get data from the input data frame Rcpp::DataFrame cohort(the_cohort); // now extract columns by name from // the data fame into C++ vectors std::vector\u0026lt;double\u0026gt; age_v = Rcpp::as\u0026lt; std::vector\u0026lt;double\u0026gt; \u0026gt;(cohort[\u0026quot;age\u0026quot;]); std::vector\u0026lt;int\u0026gt; female_v = Rcpp::as\u0026lt; std::vector\u0026lt;int\u0026gt; \u0026gt;(cohort[\u0026quot;female\u0026quot;]); std::vector\u0026lt;int\u0026gt; ily_v = Rcpp::as\u0026lt; std::vector\u0026lt;int\u0026gt; \u0026gt;(cohort[\u0026quot;ily\u0026quot;]); // create a new variable v_prob for export std::vector\u0026lt;double\u0026gt; v_prob (ily_v.size()); // iterate over data frame to calculate v_prob for (int i = 0; i \u0026lt; v_prob.size() ; i++) { v_prob[i] = vaccinate_cxx(age_v[i],female_v[i],ily_v[i]); } // export the old with the new in a combined data frame return Rcpp::DataFrame::create( Named(\u0026quot;age\u0026quot;)= age_v, Named(\u0026quot;female\u0026quot;) = female_v, Named(\u0026quot;ily\u0026quot;) = ily_v, Named(\u0026quot;p\u0026quot;) = v_prob); ' # write the helper function also in C++ # Note small change here from original to include Rcpp:NumericVector::create # for use with min and max vaccinate_cxx_src \u0026lt;- ' double vaccinate_cxx (double age, int female, int ily){ // this is based on some pretend regression equation double p = 0.25 + 0.3 * 1/(1-exp(0.004*age)) + 0.1 *ily; // use some logic p = p * (female ? 1.25 : 0.75); // boundary checking p = max(Rcpp::NumericVector::create(0,p)); p = min(Rcpp::NumericVector::create(1,p)); return(p); } ' # create an R function to call the C++ code do_rcpp \u0026lt;- cxxfunction(signature(the_cohort=\u0026quot;data.frame\u0026quot;), do_rcpp_src, plugin=\u0026quot;Rcpp\u0026quot;, includes=c('#include \u0026lt;cmath\u0026gt;', vaccinate_cxx_src))  May the best function win # benchmarking library(rbenchmark) bm_results \u0026lt;- benchmark(do_forloop(cohort), do_apply(cohort), do_plyr(cohort), do_rcpp(cohort), replications=1000) library(lattice) strategy \u0026lt;- with(bm_results, reorder(test,relative)) barchart(relative ~ strategy, bm_results, ylab='Relative performance', xlab='Strategy', main='Performance of iteration strategies over data frames in R', col='firebrick',scales=list(x=list(cex=1.2)))  Ree – donc – u – lous.\n","date":1465948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556123451,"objectID":"964ac83724d7d8267b67e2fe5538fcb0","permalink":"/post/rcpp-is-smoking-fast-for-agent-based-models-in-data-frames/","publishdate":"2016-06-15T00:00:00Z","relpermalink":"/post/rcpp-is-smoking-fast-for-agent-based-models-in-data-frames/","section":"post","summary":"Here\u0026rsquo;s the blog post originally posted on babelgraph.org on July 11, 2012. Thanks to Hadley Wickham for referencing some of content here, and apologies for the broken URL. NB. The original C++ code didn\u0026rsquo;t seem to compile on my computer today. It required a small tweak with NumericVector::create \u0026ndash; see below.\nIn particular, R data frames provide a simple framework for representing large cohorts of agents in stochastic epidemiological models, such as those representing disease transmission.","tags":[],"title":"Rcpp is smoking fast for agent based models in data frames","type":"post"}]