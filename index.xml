<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gary Weissman, MD, MSHP</title>
    <link>https://gweissman.github.io/</link>
      <atom:link href="https://gweissman.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Gary Weissman, MD, MSHP</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 06 Jan 2023 00:00:00 -0400</lastBuildDate>
    <image>
      <url>https://gweissman.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Gary Weissman, MD, MSHP</title>
      <link>https://gweissman.github.io/</link>
    </image>
    
    <item>
      <title>Global, Race-Neutral Reference Equations and Pulmonary Function Test Interpretation</title>
      <link>https://gweissman.github.io/publication/pfts_jno_2023/</link>
      <pubDate>Fri, 06 Jan 2023 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/pfts_jno_2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Net Benefit and Decision Curve Analysis in R - Some helpful functions</title>
      <link>https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/</link>
      <pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;One of the challenges of developing clinical prediction models is deciding which one is best. Sometimes traditional performance measures such as the Brier score or C-statistic might be used to quantify to what extent one model performs better than another. But in the realm of clinical medicine, a model’s predictive performance, whether measured by discrimination, calibration, or something else, doesn’t guarantee clinical benefit. When it comes to patients, that is usually the most important thing.&lt;/p&gt;
&lt;p&gt;The clinical benefit of a prediction model, however, is not straightforward to measure. How well does the model perform for an individual patient? How might delivery of the predictive information to different stakeholders influence (or not) their decision making? Does the model perform better for some groups of patients compared to others? How do individual stakeholders value different aspects of the prediction model, including different errors? How are the costs and benefits accounted for?&lt;/p&gt;
&lt;p&gt;If one wanted to answer these questions, undertaking a discrete choice experiment, an ethnographic study with qualitative interviews of all stakeholders, a human factors development and design process, and a randomized clinical trial, could help. But these studies are complex, time consuming, and costly. Is there a faster way to compare the clinical benefit of a group of prediction models?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-curve-analysis-and-net-benefit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decision curve analysis and net benefit&lt;/h2&gt;
&lt;p&gt;In 2006, &lt;a href=&#34;https://journals.sagepub.com/doi/10.1177/0272989X06295361&#34;&gt;Vickers and Elkin&lt;/a&gt; introduced the idea of Decision Curve Analysis, an approach to estimate the net benefit of a prediction model using only the data available from the model itself. That is, using only the predicted probabilities of a model along with the observed outcomes, and relying on the assumption that a decision threshold is chosen to represent the relative importance of sensitivity and specificity (or false positive and false negative predictions), the authors describe how to calculate the &lt;strong&gt;net benefit&lt;/strong&gt; of a model.&lt;/p&gt;
&lt;p&gt;In plain language, &lt;em&gt;the net benefit is the proportion of people rightly treated less the proportion wrongly treated, weighted by the decision threshold to choose treatment or not.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For a helpful discussion on interpreting decision curves see &lt;a href=&#34;https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-019-0064-7&#34;&gt;Vickers, van Calster, and Steyerberg (2019)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the original paper, the net benefit is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{TP_c}{N} - \frac{FP_c}{N} \left(\frac{p_t}{1 - p_t}\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(TP_c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(FP_c\)&lt;/span&gt; are the true- and false-positive counts, respectively, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the number of observations, and &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; is the probability threshold.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-implementation-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An implementation in R&lt;/h2&gt;
&lt;p&gt;Here is an implementation of the net benefit calculation that is implemented in the &lt;a href=&#34;https://github.com/gweissman/gmish&#34;&gt;gmish&lt;/a&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb &amp;lt;- function(preds, obs, p_t, weight = 1) {
  weight * tpc(preds, obs, thresh = p_t) / length(preds) - fpc(preds, obs, thresh = p_t) / length(preds) * p_t / (1 - p_t)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;tpc&lt;/code&gt; and &lt;code&gt;fpc&lt;/code&gt; functions provide the true- and false-positive counts at the threshold &lt;code&gt;p_t&lt;/code&gt;. This function also allows further weighting but we’ll save that for a later discussion. The default &lt;code&gt;weight = 1&lt;/code&gt; reduces to the original equation described above. To calculate the net benefit over the full range of thresholds and plot a decision curve, here is another function also implemented in the same packae:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_plot &amp;lt;- function(form, data, treat_all = TRUE, treat_none = TRUE, omniscient = TRUE, weight = 1, max_neg = 0.1) {

  data &amp;lt;- as.data.table(data)
  # Identify vars
  .y &amp;lt;- all.vars(form)[1]
  .mods &amp;lt;- all.vars(form)[-1]

  dt_list &amp;lt;- lapply(.mods, function(m) {
    data.table(p_t = seq(0,0.99,.01))[,
                                                               .(Model = m,
                                                                 net_benefit = nb(data[,get(m)],
                                                                                       data[,get(.y)],
                                                                                       p_t = p_t,
                                                                                       weight = weight)),
                                                               by = p_t]
  })

  if (treat_all) {
    treat_all_dt &amp;lt;- data.table(p_t = seq(0,0.99,.01))[,
                                      .(Model = &amp;#39;Treat all&amp;#39;,
                                        net_benefit = nb(rep(1, nrow(data)), # guess 1 for everyone
                                                         data[,get(.y)],
                                                         p_t = p_t,
                                                         weight = weight)),
                                      by = p_t]
    dt_list &amp;lt;- append(dt_list, list(treat_all_dt))
  }

  if (treat_none) {
    treat_none_dt &amp;lt;- data.table(Model = &amp;#39;Treat none&amp;#39;, p_t = seq(0,0.99,.01), net_benefit = 0)
    dt_list &amp;lt;- append(dt_list, list(treat_none_dt))
  }

  if (omniscient) {
    omniscient_dt &amp;lt;- data.table(p_t = seq(0,0.99,.01))[,
                                                      .(Model = &amp;#39;Treat omnisciently&amp;#39;,
                                                        net_benefit = nb(data[,get(.y)],
                                                                         data[,get(.y)],
                                                                         p_t = p_t,
                                                                         weight = weight)),
                                                      by = p_t]
    dt_list &amp;lt;- append(dt_list, list(omniscient_dt))
  }

  dt_all &amp;lt;- rbindlist(dt_list, use.names = TRUE)

  ymax &amp;lt;- max(dt_all$net_benefit, na.rm = TRUE)

  p &amp;lt;- ggplot(dt_all, aes(p_t, net_benefit, color = Model)) +
    geom_line(size = 0.3) +
    xlim(0, 1) + ylim(-1 * max_neg * ymax, ymax * 1.05) +
    xlab(&amp;#39;Threshold probability&amp;#39;) + ylab(&amp;#39;Net benefit&amp;#39;) +
    theme_bw() +
    coord_fixed()

  return(p)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function accepts a formula &lt;code&gt;form&lt;/code&gt; to indicate the outcome variable and then several models to compare simultaneously. Additional decision curves can be added to the plot for &lt;code&gt;treat_all&lt;/code&gt;, &lt;code&gt;treat_none&lt;/code&gt;, and &lt;code&gt;omniscient&lt;/code&gt; treatment strategies. This calculation is really the benefit accrued to those treated and so the benefit of treating none is by definition zero (and so does not account for the potential costs of a false negative error, i.e. failing to treat someone who could have benefited from treatment).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-a-completely-invented-case-study&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example (A completely invented case study)&lt;/h2&gt;
&lt;p&gt;Let’s say we have a bunch of penguins from the &lt;code&gt;palmerpenguins&lt;/code&gt; dataset all hanging out together. Sadly, the water has been contaminated on the island of Biscoe and all of penguins from that island need an urgent antidote to prevent them from falling ill. However, the local ecologist doesn’t know which island the penguins came from and instead can only measure their bills, flippers, body mass, and sex. Let’s build some classification models to help the ecologist find penguins who need treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries and data
library(gmish) # remotes::install_github(&amp;#39;gweissman/gmish&amp;#39;) if not installed already
library(data.table)
library(ggplot2)
library(palmerpenguins)
library(ranger)
library(ggsci)

# Use complete cases for the example
pp &amp;lt;- penguins[complete.cases(penguins),]

# Take a look at the data

ggplot(pp, aes(bill_length_mm, flipper_length_mm, size = body_mass_g, color = sex)) +
  geom_point() +
  facet_grid(~island) +
  theme_bw() +
  scale_color_aaas()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/index.en_files/figure-html/example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Build a model
model1 &amp;lt;- glm(island == &amp;#39;Biscoe&amp;#39; ~ bill_length_mm + flipper_length_mm + body_mass_g + sex, 
              family = binomial, data = pp)
summary(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = island == &amp;quot;Biscoe&amp;quot; ~ bill_length_mm + flipper_length_mm + 
##     body_mass_g + sex, family = binomial, data = pp)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7141  -0.6526  -0.1626   0.3955   2.4611  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)       -1.417e+01  3.380e+00  -4.192 2.77e-05 ***
## bill_length_mm    -1.620e-01  4.267e-02  -3.796 0.000147 ***
## flipper_length_mm  5.012e-02  2.562e-02   1.956 0.050416 .  
## body_mass_g        2.897e-03  5.061e-04   5.725 1.04e-08 ***
## sexmale           -1.692e+00  4.020e-01  -4.208 2.57e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 461.49  on 332  degrees of freedom
## Residual deviance: 254.21  on 328  degrees of freedom
## AIC: 264.21
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds_model1 &amp;lt;- predict(model1, type = &amp;#39;response&amp;#39;)
# How well does it perform?
make_perf_df(preds_model1, pp$island == &amp;#39;Biscoe&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   metric   estimate      ci_lo     ci_hi
## 1  brier 0.11931004 0.09459123 0.1414849
## 2 sbrier 0.52254884 0.44031013 0.6204561
## 3    ici 0.08269116 0.04607298 0.1093371
## 4  lloss 0.38170012 0.31786040 0.4392324
## 5  cstat 0.88448214 0.84624249 0.9213535&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Build a second model
model2 &amp;lt;- ranger(island == &amp;#39;Biscoe&amp;#39; ~ bill_length_mm + flipper_length_mm + body_mass_g + sex, 
               data = pp, probability = TRUE)
print(model2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger(island == &amp;quot;Biscoe&amp;quot; ~ bill_length_mm + flipper_length_mm +      body_mass_g + sex, data = pp, probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  500 
## Sample size:                      333 
## Number of independent variables:  4 
## Mtry:                             2 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.1107188&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds_model2 &amp;lt;- predict(model2, data = pp)$predictions[,2]
make_perf_df(preds_model2, pp$island == &amp;#39;Biscoe&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   metric   estimate      ci_lo      ci_hi
## 1  brier 0.04343418 0.03476987 0.05138225
## 2 sbrier 0.82618645 0.79477513 0.86025131
## 3    ici 0.09017145 0.07128279 0.10680068
## 4  lloss 0.16303220 0.14019543 0.18625625
## 5  cstat 0.99862865 0.99736560 1.00072050&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Summary predictions
results_dt &amp;lt;- data.table(sick_island = pp$island == &amp;#39;Biscoe&amp;#39;,
                         glm = preds_model1,
                         rf = preds_model2)

# Look at model precision and recall
pr_plot(sick_island ~ glm + rf, data = results_dt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (`geom_point()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (`geom_line()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/index.en_files/figure-html/example-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Look at model calibration
calib_plot(sick_island ~ glm + rf, data = results_dt, cuts = 5, rug = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 4 rows containing missing values (`geom_bar()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/index.en_files/figure-html/example-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we have a general sense of the models’ performance, which seems pretty good overall. The fact that they are all likely very overfit in this example is important to mention but we’ll deal with that in a later conversation. Now, we’re stuck with the question, which model should we use to pick out the penguins for treatment? The regression model or the random forest model?&lt;/p&gt;
&lt;p&gt;The intended use case for decision curves is to identify the optimal model &lt;em&gt;given&lt;/em&gt; a choice of threshold. In this case, let’s say that the medication does have some but not terrible side effects, so we prefer a decision threshold around 30%. Which model provides the most benefit in this range?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For simplicity, just plot the net benefit of the two models trained above
nb_plot(sick_island ~ glm + rf, data = results_dt, 
        treat_all = FALSE, treat_none = FALSE, omniscient = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/index.en_files/figure-html/calcdca-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly in the 30% threshold range, the random forest model provides more benefit to those treated. Now let’s compare these models against treat all, treat none, and treat omnisciently strategies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_plot(sick_island ~ glm + rf, data = results_dt, 
        treat_all = TRUE, treat_none = TRUE, omniscient = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 48 rows containing missing values (`geom_line()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/index.en_files/figure-html/calcdca2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The treat none strategy, by definition, provides zero net benefit, so represents a lower bound of what the other models should provide if they are useful. The treat omnisciently strategy represents the upper bound on what any model could provide. In this case, the treat all strategy does a little better than the glm model in the 10 to 20% threshold range, but otherwise provides less net benefit than both the glm and rf across the rest of the range.&lt;/p&gt;
&lt;p&gt;Let’s again highlight the range of interest that we determined ahead of time based on our choice of threshold which here determines the relative weighting of true and false positives. However, if you asked the penguins, they might choose a different threshold!&lt;/p&gt;
&lt;p&gt;Since we chose 30% as our decision threshold, let’s examine the neighborhood around 30% looking at 5% in either direction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_plot(sick_island ~ glm + rf, data = results_dt, 
        treat_all = TRUE, treat_none = TRUE, omniscient = TRUE) &amp;amp;
  geom_rect(xmin = 0.25, xmax = 0.35, ymin = -Inf, ymax = Inf, 
            fill = &amp;#39;orange&amp;#39;, color = &amp;#39;orange&amp;#39;, alpha = 0.01, inherit.aes = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 48 rows containing missing values (`geom_line()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/net-benefit-and-decision-curve-analysis-in-r-some-helpful-functions/index.en_files/figure-html/calcdca3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this range of interest, the random forest model provides the highest net benefit. Let’s go save some penguins!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using MetaMap with Python to access the UMLS MetaThesaurus - A Quick Start Guide</title>
      <link>https://gweissman.github.io/post/using-metamap-with-python-to-access-the-umls-metathesaurus-a-quick-start-guide/</link>
      <pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/using-metamap-with-python-to-access-the-umls-metathesaurus-a-quick-start-guide/</guid>
      <description>&lt;p&gt;The National Library of Medicine (NLM) created a tremendous resource called the &lt;a href=&#34;https://www.nlm.nih.gov/research/umls/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unified Medical Language Systems (UMLS)&lt;/a&gt;. The UMLS is a suite of data and software resources that offers several tools for processing, representing, and analyzing biomedical text data. One exciting use case that we&amp;rsquo;ll explore here is using UMLS to identify signs and symptoms that are documented in the text of clinical encounter notes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lhncbc.nlm.nih.gov/ii/tools/MetaMap.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MetaMap&lt;/a&gt; is a software tool that identifies concepts in the text of clinical notes that are found in UMLS ontologies. MetaMap is written in Java and there is a convenient wrapper in Python:  &lt;a href=&#34;https://github.com/AnthonyMRios/pymetamap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMetaMap&lt;/a&gt; written by Anthony Rios. There are other Python wrappers around MetaMap but PyMetaMap was the most straightforward to get running on my machine so that&amp;rsquo;s what we&amp;rsquo;ll use here.&lt;/p&gt;
&lt;h2 id=&#34;step-1-installation&#34;&gt;Step 1: Installation&lt;/h2&gt;
&lt;p&gt;Make sure you have have MetaMap installed. Instructions are here: &lt;a href=&#34;https://metamap.nlm.nih.gov/Installation.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://metamap.nlm.nih.gov/Installation.shtml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I used the default settings for the installation and it has worked out fine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; To use most UMLS resources you will have to &lt;a href=&#34;https://uts.nlm.nih.gov/uts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;register for an account&lt;/a&gt; and sign a data use agreement. This is free!&lt;/p&gt;
&lt;h2 id=&#34;step-2-get-some-text-data&#34;&gt;Step 2: Get some text data&lt;/h2&gt;
&lt;p&gt;Because working with clinical notes is often challening due to issues of de-identification, here are a few sample text notes we&amp;rsquo;ll use here. These are completely made up and not based on any real people.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;note1 = &amp;quot;Mrs. Jones came in today complaining of a lot of chest pain. She denies shortness of breath and fevers.&amp;quot;

note2 = &amp;quot;Mr. Smith has been having pain in his knee for many weeks. It hurts when he walks. There is no swelling, erythema, or micromotion tenderness.&amp;quot;

note3 = &amp;quot;Sandy Lemon has been having headaches for two months that are associated with blurry vision and numbness in her right arm.&amp;quot;

# Put them all together in a list
note_list = [note1, note2, note3]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;step-3-install-pymetamap&#34;&gt;Step 3: Install PyMetaMap&lt;/h2&gt;
&lt;p&gt;The last time I tried this, the version available via &lt;code&gt;pip&lt;/code&gt; was old. So installed the latest version (0.2) directly from the GitHub site here: &lt;a href=&#34;https://github.com/AnthonyMRios/pymetamap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/AnthonyMRios/pymetamap&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I accomplished this using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;pip install git+https://github.com/AnthonyMRios/pymetamap.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now load the necessary modules and set up the relevant servers in the background:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;# Load MetaMap
from pymetamap import MetaMap

# Import os to make system calls
import os

# For pausing
from time import sleep

# Setup UMLS Server
metamap_base_dir = &#39;/gwshare/umls_2021/metamap/public_mm/&#39;
metamap_bin_dir = &#39;bin/metamap20&#39;
metamap_pos_server_dir = &#39;bin/skrmedpostctl&#39;
metamap_wsd_server_dir = &#39;bin/wsdserverctl&#39;

# Start servers
os.system(metamap_base_dir + metamap_pos_server_dir + &#39; start&#39;) # Part of speech tagger
os.system(metamap_base_dir + metamap_wsd_server_dir + &#39; start&#39;) # Word sense disambiguation 

# Sleep a bit to give time for these servers to start up
sleep(60)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set up a MetaMap object to do the work. If you are running into memory issues with this approach, see some tips here: &lt;a href=&#34;https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/OutOfMemory.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/OutOfMemory.pdf&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;metam = MetaMap.get_instance(metamap_base_dir + metamap_bin_dir)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s process some text.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;cons, errs = metam.extract_concepts(note_list,
                                word_sense_disambiguation = True,
                                restrict_to_sts = [&#39;sosy&#39;], # signs and symptoms
                                composite_phrase = 1, # for memory issues
                                prune = 30)
                                
# Look at the output:
print(cons)

## [ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;5.18&#39;, preferred_name=&#39;Dyspnea&#39;, cui=&#39;C0013404&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;SHORTNESS OF BREATH&amp;quot;-tx-1-&amp;quot;shortness of breath&amp;quot;-noun-1]&#39;, location=&#39;TX&#39;, pos_info=&#39;73/19&#39;, tree_codes=&#39;&#39;), ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;5.18&#39;, preferred_name=&#39;Fever&#39;, cui=&#39;C0015967&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;Fevers&amp;quot;-tx-1-&amp;quot;fevers&amp;quot;-noun-1]&#39;, location=&#39;TX&#39;, pos_info=&#39;97/6&#39;, tree_codes=&#39;&#39;), ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;3.45&#39;, preferred_name=&#39;Mastodynia&#39;, cui=&#39;C0024902&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;of breast pain&amp;quot;-tx-1-&amp;quot;of chest pain&amp;quot;-noun-0]&#39;, location=&#39;TX&#39;, pos_info=&#39;38/2,50/10&#39;, tree_codes=&#39;&#39;), ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;3.71&#39;, preferred_name=&#39;Knee pain&#39;, cui=&#39;C0231749&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;Pain in knee&amp;quot;-tx-1-&amp;quot;pain in knee&amp;quot;-noun-0]&#39;, location=&#39;TX&#39;, pos_info=&#39;27/7,39/4&#39;, tree_codes=&#39;&#39;), ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;3.68&#39;, preferred_name=&#39;Sore to touch&#39;, cui=&#39;C0234233&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;TENDERNESS (NOS)&amp;quot;-tx-1-&amp;quot;tenderness&amp;quot;-noun-1]&#39;, location=&#39;TX&#39;, pos_info=&#39;131/10&#39;, tree_codes=&#39;&#39;), ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;3.57&#39;, preferred_name=&#39;Headache&#39;, cui=&#39;C0018681&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;Headaches&amp;quot;-tx-1-&amp;quot;headaches&amp;quot;-noun-0]&#39;, location=&#39;TX&#39;, pos_info=&#39;29/9&#39;, tree_codes=&#39;&#39;), ConceptMMI(index=&#39;USER&#39;, mm=&#39;MMI&#39;, score=&#39;3.56&#39;, preferred_name=&#39;Numbness&#39;, cui=&#39;C0028643&#39;, semtypes=&#39;[sosy]&#39;, trigger=&#39;[&amp;quot;NUMBNESS&amp;quot;-tx-1-&amp;quot;numbness&amp;quot;-noun-0]&#39;, location=&#39;TX&#39;, pos_info=&#39;97/8&#39;, tree_codes=&#39;&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And try to put it into a more usable format:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{python}&#34;&gt;import pandas as pd

def get_keys_from_mm(concept, klist):
    conc_dict = concept._asdict()
    conc_list = [conc_dict.get(kk) for kk in klist]
    return(tuple(conc_list))
        
        
keys_of_interest = [&#39;preferred_name&#39;, &#39;cui&#39;, &#39;semtypes&#39;, &#39;pos_info&#39;]
cols = [get_keys_from_mm(cc, keys_of_interest) for cc in cons]
results_df = pd.DataFrame(cols, columns = keys_of_interest)

# See results
print(results_df)

## preferred_name       cui semtypes    pos_info
## 0        Dyspnea  C0013404   [sosy]       73/19
## 1          Fever  C0015967   [sosy]        97/6
## 2     Mastodynia  C0024902   [sosy]  38/2,50/10
## 3      Knee pain  C0231749   [sosy]   27/7,39/4
## 4  Sore to touch  C0234233   [sosy]      131/10
## 5       Headache  C0018681   [sosy]        29/9
## 6       Numbness  C0028643   [sosy]        97/8

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there you found the signs and symptoms with their associated plain language name, cui, and position. For processing many notes in parallel, I&amp;rsquo;ve found it works to load the UMLS servers once but create a new &lt;code&gt;MetaMap&lt;/code&gt; object inside each process.&lt;/p&gt;
&lt;p&gt;Good luck, and happy processing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Development and validation of a prediction model for actionable aspects of frailty in the text of clinicians’ encounter notes</title>
      <link>https://gweissman.github.io/publication/frailty_jamia_2021/</link>
      <pubDate>Sat, 13 Nov 2021 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/frailty_jamia_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Dangerous Myth: Does Speaking Imply Breathing?</title>
      <link>https://gweissman.github.io/publication/speaking_breathing/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/speaking_breathing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Locally Informed Simulation to Predict Hospital Capacity Needs During the COVID-19 Pandemic</title>
      <link>https://gweissman.github.io/publication/chime/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/chime/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparison of bootstrapping approaches for identifying variance of predictive performance</title>
      <link>https://gweissman.github.io/post/comparison-of-bootstrapping-approaches-for-identifying-variance-of-predictive-performance/</link>
      <pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/comparison-of-bootstrapping-approaches-for-identifying-variance-of-predictive-performance/</guid>
      <description>
&lt;script src=&#34;https://gweissman.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Bootstrapping is a helpful technique for identifying the variance of an estimate in a given sample when no other data are available. In the case of the evaluation of clinical prediction models, bootstrapping can be used to estimate confidence intervals around performance metrics such as the Brier score, the c-statistic, and others. When the original model and its tuning parameters, and the original dataset are available, the data can be resampled and the model refit on each replicate to make predictions. These predictions can then be used to calculate a performance metric. However, sometimes only the predicted probabilities of the original model on the original dataset are available. In this case, the predicted probabilities can be resampled and the performance metric can be calculated on each of these replicates. How do these approaches compare?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build a model&lt;/h2&gt;
&lt;p&gt;Similar to the model built in &lt;a href=&#34;https://gweissman.github.io/post/evaluating-the-equivalence-of-different-formulations-of-the-scaled-brier-score/&#34;&gt;another blog post about the Brier Score&lt;/a&gt;, let’s build a model with the abalone dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set seed
set.seed(24601)

# load libraries for plotting
library(ggplot2)

# get data
df &amp;lt;- read.csv(&amp;#39;https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data&amp;#39;)
names(df) &amp;lt;- c(&amp;#39;sex&amp;#39;, &amp;#39;length&amp;#39;, &amp;#39;diameter&amp;#39;, &amp;#39;height&amp;#39;, &amp;#39;weight_whole&amp;#39;, 
               &amp;#39;weight_shucked&amp;#39;, &amp;#39;weight_viscera&amp;#39;, &amp;#39;weight_shell&amp;#39;, &amp;#39;rings&amp;#39;)
# inspect data
knitr::kable(head(df))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;diameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;height&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_whole&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_shucked&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_viscera&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_shell&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.350&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.090&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0995&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0485&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.070&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.420&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.135&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6770&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.210&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.440&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.365&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2155&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.155&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.080&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2050&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0895&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0395&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.425&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.095&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0775&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7775&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2370&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s predict whether or not an abalone will have &amp;gt; 10 rings
m &amp;lt;- glm(I(rings &amp;gt; 10) ~ ., data = df, family = binomial)
preds_m &amp;lt;- predict(m, type = &amp;#39;response&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-metric&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance metric&lt;/h2&gt;
&lt;p&gt;Let’s calculate the Brier score as a measure of model performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brier_score &amp;lt;- function(preds, obs) {
  mean((obs - preds)^2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;boostrapping-the-predicted-probabilities-only&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boostrapping the predicted probabilities only&lt;/h2&gt;
&lt;p&gt;Now let’s estimate the 95% confidence interval of the Brier score using only the predicted probabilities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 1000

get_boot_est_preds &amp;lt;- function(preds, obs, metric) {
  idx &amp;lt;- sample(length(preds), replace = TRUE)
  metric(preds[idx], obs[idx])
}

reps_pred &amp;lt;- replicate(N, get_boot_est_preds(preds_m, df$rings &amp;gt; 10, brier_score))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrapping-to-refit-the-model-on-each-replicate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrapping to refit the model on each replicate&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_boot_est_mod &amp;lt;- function(df, metric) {
    idx &amp;lt;- sample(nrow(df), replace = TRUE)
    m_b &amp;lt;- glm(I(rings &amp;gt; 10) ~ ., data = df[idx,], family = binomial)
    preds_m_b &amp;lt;- predict(m_b, type = &amp;#39;response&amp;#39;)
    metric(preds_m_b, df[idx,]$rings &amp;gt; 10)
}

reps_model &amp;lt;- replicate(N, get_boot_est_mod(df, brier_score))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;optimism-corrected-bootstrapping-to-refit-the-model-on-each-replicate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimism corrected bootstrapping to refit the model on each replicate&lt;/h2&gt;
&lt;p&gt;As pointed out by Ewout Steyerberg, these estimates should be optimism-corrected for potential overfitting. Let’s look at the difference in estimates if we make predictions on the initial dataset rather than on the bootstrapped sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_boot_est_mod &amp;lt;- function(df, metric) {
    idx &amp;lt;- sample(nrow(df), replace = TRUE)
    m_b &amp;lt;- glm(I(rings &amp;gt; 10) ~ ., data = df[idx,], family = binomial)
    preds_m_b &amp;lt;- predict(m_b, data = df, type = &amp;#39;response&amp;#39;)
    metric(preds_m_b, df$rings &amp;gt; 10)
}

reps_model_opt &amp;lt;- replicate(N, get_boot_est_mod(df, brier_score))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluate results&lt;/h2&gt;
&lt;p&gt;Let’s look at the distrbution of the Brier score estimates using both approaches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- rbind(data.frame(brier_score = reps_pred,
                        approach = &amp;#39;predictions&amp;#39;),
             data.frame(brier_score = reps_model,
                        approach = &amp;#39;refit_model&amp;#39;),
             data.frame(brier_score = reps_model_opt,
                        approach = &amp;#39;refit_opt&amp;#39;))

# Plot distribution of bootstrapped Brier scores
ggplot(res, aes(brier_score, color = approach)) + 
  geom_density() +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gweissman.github.io/post/2019-04-25-comparison-of-bootstrapping-approaches-for-identifying-variance-of-predictive-performance_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can calculate 95% confidence intervals using a simple percentile approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_ci_95 &amp;lt;- function(v) {
  q &amp;lt;- format(quantile(v, probs = c(0.025, 0.975)), digits = 5)
  paste0(&amp;#39;(&amp;#39;,q[1],&amp;#39; to &amp;#39;,q[2],&amp;#39;)&amp;#39;)
  }

cat(&amp;#39;CI using bootstrapped estimates from predictions only:&amp;#39;,
    calc_ci_95(reps_pred),&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CI using bootstrapped estimates from predictions only: (0.14153 to 0.15414)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;#39;CI using bootstrapped estimates from refitting the model:&amp;#39;,
    calc_ci_95(reps_model),&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CI using bootstrapped estimates from refitting the model: (0.14128 to 0.15395)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;#39;CI using bootstrapped estimates from refitting the model with an optimism correction:&amp;#39;,
    calc_ci_95(reps_model_opt),&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CI using bootstrapped estimates from refitting the model with an optimism correction: (0.29691 to 0.31728)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;While not exactly the same, the first two approaches yield very similar results. However, this outcome may vary depending on potential bias in the original dataset, and the sensitivity of the model to such bias. The third approach demonstrates the likely high degree of overfitting in this model, and the need to properly correct for optimism when reporting predictive performance in the absence of a separate testing sample.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating the equivalence of different formulations of the scaled Brier score</title>
      <link>https://gweissman.github.io/post/evaluating-the-equivalence-of-different-formulations-of-the-scaled-brier-score/</link>
      <pubDate>Tue, 23 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/evaluating-the-equivalence-of-different-formulations-of-the-scaled-brier-score/</guid>
      <description>
&lt;script src=&#34;https://gweissman.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The Brier Score is a composite measure of discrimination and calibration for a prediction model. The Brier Score is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
BS = \frac{1}{N} \sum (y_i - \hat y_i)^2,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the number of observations, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is the observed outcome, either 0 or 1, and &lt;span class=&#34;math inline&#34;&gt;\(\hat y_i\)&lt;/span&gt; is the predicted probability for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th observation. Let’s create an R function to calculate the Brier Score:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brier_score &amp;lt;- function(obs, pred) {
  mean((obs - pred)^2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The scaled Brier Score accounts for the event rate and provides an immediate comparison to an uninformative model that is equivalent to “just guess the event rate.” An intuitive way to define the scaled Brier Score (also called the “Brier skill score”) is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
BS_{scaled} = 1 - \frac{BS}{BS_{max}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(BS_{max} = \frac{1}{N} \sum (y_i - \bar y_i)^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y_i\)&lt;/span&gt; is the event rate among the observed outcome.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-confusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My confusion&lt;/h2&gt;
&lt;p&gt;This formulation of the scaled Brier Score makes intuitive sense to me and is how I go about calculating it in practice. However, two other distinct formulations have been proposed for calculating &lt;span class=&#34;math inline&#34;&gt;\(BS_{max}\)&lt;/span&gt; that — at least to the limits of my algebraic skills – differ. Thus, here I proposed a numeric investigation of these different definitions to see if they are indeed equivalent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition 1&lt;/h2&gt;
&lt;p&gt;This is the intuitive definition to which I am accustomed, and is made explicit here: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/29713202&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/29713202&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
BS_{scaled} = 1 - \frac{\frac{1}{N} \sum (y_i - \hat y_i)^2}{\frac{1}{N} \sum (y_i - \bar y_i)^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s create an R function to calculate this value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_1 &amp;lt;- function(obs, pred) {
  1 - (brier_score(obs, pred) / brier_score(obs, mean(obs)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition 2&lt;/h2&gt;
&lt;p&gt;A second formulation of the scaled Brier Score is defined with a slightly different definition of &lt;span class=&#34;math inline&#34;&gt;\(BS_{max}\)&lt;/span&gt;, which is in this case described in &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/20010215&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/20010215&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
BS_{max} = \bar p \times (1 - \bar p).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s create an R function to calculate this measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_2 &amp;lt;- function(obs, pred) {
  1 - (brier_score(obs, pred) / (mean(obs) * (1 - mean(obs))))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition 3&lt;/h2&gt;
&lt;p&gt;A third formulation of the scaled Brier Score is defined with a slightly different definition of &lt;span class=&#34;math inline&#34;&gt;\(BS_{max}\)&lt;/span&gt;, which is in this case described in &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/22961910&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/22961910&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
BS_{max} = \bar p \times (1 - \bar p)^2 + (1 - \bar p) \times \bar p^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s create an R function to calculate this measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_3 &amp;lt;- function(obs, pred) {
  1 - (brier_score(obs, pred) / (mean(obs) * (1 - mean(obs))^2 + (1 - mean(obs)) * mean(obs)^2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;build-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build a model&lt;/h2&gt;
&lt;p&gt;Let’s build a sample model based on the UCI abalone data (&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Abalone&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Abalone&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get data
df &amp;lt;- read.csv(&amp;#39;https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data&amp;#39;)
names(df) &amp;lt;- c(&amp;#39;sex&amp;#39;, &amp;#39;length&amp;#39;, &amp;#39;diameter&amp;#39;, &amp;#39;height&amp;#39;, &amp;#39;weight_whole&amp;#39;, 
               &amp;#39;weight_shucked&amp;#39;, &amp;#39;weight_viscera&amp;#39;, &amp;#39;weight_shell&amp;#39;, &amp;#39;rings&amp;#39;)
# inspect data
knitr::kable(head(df))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;diameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;height&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_whole&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_shucked&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_viscera&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;weight_shell&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.350&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.090&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0995&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0485&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.070&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.420&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.135&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6770&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2565&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.210&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;M&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.440&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.365&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2155&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.155&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.080&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2050&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0895&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0395&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;I&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.425&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.095&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0775&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.120&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;F&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7775&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2370&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s predict whether or not an abalone will have &amp;gt; 10 rings
m1 &amp;lt;- glm(I(rings &amp;gt; 10) ~ ., data = df, family = binomial)
preds_m1 &amp;lt;- predict(m1, type = &amp;#39;response&amp;#39;)

# And another model with severe class imablance
m2 &amp;lt;- glm(I(rings &amp;gt; 3) ~ ., data = df, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds_m2 &amp;lt;- predict(m2, type = &amp;#39;response&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;score-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Score the model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ---------- Model 1
# Calculate the Brier Score
brier_score(df$rings &amp;gt; 10, preds_m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1479862&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate each type of scaled Brier Score
scaled_brier_score_1(df$rings &amp;gt; 10, preds_m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3462507&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_2(df$rings &amp;gt; 10, preds_m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3462507&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_3(df$rings &amp;gt; 10, preds_m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3462507&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ---------- Model 2
# Calculate the Brier Score
brier_score(df$rings &amp;gt; 3, preds_m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002690905&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate each type of scaled Brier Score
scaled_brier_score_1(df$rings &amp;gt; 3, preds_m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3362851&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_2(df$rings &amp;gt; 3, preds_m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3362851&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scaled_brier_score_3(df$rings &amp;gt; 3, preds_m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3362851&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Construct validity of six sentiment analysis methods in the text of encounter notes of patients with critical illness</title>
      <link>https://gweissman.github.io/publication/sentiment_2019/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/sentiment_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Condition Categories for Pulmonary Diseases: Population Health Management and Policy Opportunities.</title>
      <link>https://gweissman.github.io/publication/hccs_2019/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/hccs_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Potentially Preventable Intensive Care Unit Admissions in the United States, 2006 - 2015.</title>
      <link>https://gweissman.github.io/publication/pot_prev_2019/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/pot_prev_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inclusion of unstructured clinical text improves early prediction of death or prolonged ICU stay</title>
      <link>https://gweissman.github.io/publication/text_preds_2019/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 -0400</pubDate>
      <guid>https://gweissman.github.io/publication/text_preds_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Building a recurrent neural network to predict time-series data with Keras in Python</title>
      <link>https://gweissman.github.io/post/building-a-recurrent-neural-network-to-predict-time-series-data-with-keras-in-python/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/building-a-recurrent-neural-network-to-predict-time-series-data-with-keras-in-python/</guid>
      <description>&lt;p&gt;Recurrent neural networks and their variants are helpful for extracting information from time series. Here&amp;rsquo;s an example using sample data to get up and running.&lt;/p&gt;
&lt;p&gt;I found the following other websites helpful in reading up on code examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_benchmark.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_benchmark.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction/blob/master/lstm.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction/blob/master/lstm.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# setup
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Dropout, SimpleRNN
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

# make a sample multivariable time series - not autoregressive
# generate a random walk
def random_walk(steps, scale = 1):
    w = np.zeros(steps)
    for x in range(1,steps):
        w[x] = w[x-1] + scale * np.random.normal()
    return(w)
        
time_steps = 5000
data = pd.DataFrame({&#39;x&#39; : range(time_steps), 
    &#39;y&#39; : np.arange(time_steps) ** (1/2) + random_walk(time_steps) })
data = data.assign(z = np.log(data.x+1) + 0.3 * data.y)
data_mat = np.array(data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at the data.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/rnn_data_explore.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.subplot(2,1,1)
plt.plot(data_mat[:,0], data_mat[:,2], c = &#39;goldenrod&#39;)
plt.margins(0.05)
plt.subplot(2,1,2)
plt.plot(data_mat[:,1], data_mat[:,2], c = &#39;firebrick&#39;)
plt.margins(0.05)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# split into samples (sliding time windows)
samples = list()
target = list()
length = 50

# step over the 5,000 in jumps of length
for i in range(time_steps - length - 1):
  # grab from i to i + length
    sample = data_mat[i:i+length,:2]
    outcome = data_mat[i+length+1,2]
    target.append(outcome)
    samples.append(sample)

# split out a test set
test_size = 1000
x_test_mat = np.dstack(samples[-test_size:])
x_test_3d_final = np.moveaxis(x_test_mat, [0, 1, 2], [1, 2, 0] )

# The RNN needs data with the format of [samples, time steps, features].
# Here, we have N samples, 50 time steps per sample, and 2 features
data_mat_stacked = np.dstack(samples[:-test_size])
data_mat_3d_final = np.moveaxis(data_mat_stacked, [0, 1, 2], [1, 2, 0] )

# and fix up the target
target_arr = np.array(target[:-test_size])

# now build the RNN
model = Sequential()
model.add(SimpleRNN(128, input_shape = (data_mat_3d_final.shape[1],
    data_mat_3d_final.shape[2]), activation = &#39;relu&#39;))
model.add(Dropout(0.1))
model.add(Dense(64, activation = &#39;relu&#39;))
model.add(Dropout(0.1))
model.add(Dense(16, activation = &#39;relu&#39;))
model.add(Dropout(0.1))
model.add(Dense(1, activation=&#39;linear&#39;))

# monitor validation progress
early = EarlyStopping(monitor = &amp;quot;val_loss&amp;quot;, mode = &amp;quot;min&amp;quot;, patience = 7)
callbacks_list = [early]
    
model.compile(loss = &#39;mean_squared_error&#39;,
              optimizer = &#39;adam&#39;,
              metrics = [&#39;mse&#39;])

# and train the model
history = model.fit(data_mat_3d_final, target_arr, 
    epochs=50, batch_size=25, verbose=2, 
    validation_split = 0.20,
    callbacks = callbacks_list)

# plot history
plt.plot(history.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;val&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/rnn_train_history.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# get predictions
train_predictions = model.predict(data_mat_3d_final)
test_predictions = model.predict(x_test_3d_final)

# plot predictions vs actual
plt.plot(data[&#39;x&#39;], 
    data[&#39;z&#39;], c = &#39;goldenrod&#39;, label = &#39;data&#39;)
plt.plot(data.iloc[(length+1):-test_size][&#39;x&#39;], 
    train_predictions, c = &#39;navy&#39;, label = &#39;train&#39;)
plt.plot(data.iloc[-test_size:][&#39;x&#39;], 
    test_predictions, c = &#39;firebrick&#39;, label = &#39;test&#39;)
plt.legend(loc=&#39;best&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/rnn_times_series_predictions.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Not bad!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making choropleth maps by zip code</title>
      <link>https://gweissman.github.io/post/making-choropleth-maps-by-zip-code/</link>
      <pubDate>Mon, 17 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/making-choropleth-maps-by-zip-code/</guid>
      <description>&lt;p&gt;Every now and then it is useful to make a map. In times of political uncertainty, data can light a path forward. Your local elected officals may be interested in data, too, and how they impact policy (and re-election, of course).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say you wanted to know if people are still enrolling the &lt;em&gt;Healthcare.gov&lt;/em&gt; plans in your local State Senatorial district. It&amp;rsquo;s quite difficult to find data broken up at this level but zip codes can approximate such an approach. Here&amp;rsquo;s information from Pennsylvania 26th Senatorial District.&lt;/p&gt;
&lt;h2 id=&#34;marketplace-enrollment&#34;&gt;Marketplace enrollment&lt;/h2&gt;
&lt;p&gt;Using publicly available data from the Centers for Medicare &amp;amp; Medicaid Services, this file includes data by zip code on enrollment through &lt;a href=&#34;https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Marketplace-Products/Plan_Selection_ZIP.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;HealthCare.gov&lt;/em&gt;&lt;/a&gt; The data description reads:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;2017 OEP ZIP Code-Level Public Use File: This ZIP code and APTC PUF includes total health plan selections, the count of consumers with APTC, and average APTC among consumers with APTC between November 1, 2016 and January 31, 2017. This PUF only includes data for the 39 states that used the HealthCare.gov platform in 2017.&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# setup
require(zipcode)
library(readxl)
require(dplyr)
require(choroplethrZip)
require(magrittr)


data(zipcode)
# download and extract this file: https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Marketplace-Products/Plan_Selection_ZIP.html

dt &amp;lt;- read_excel(&#39;2017_OEP_ZIP_Code-Level_Public_Use_File.xlsx&#39;, sheet = 4)

# approximating the 26th district
zips_26thsenate &amp;lt;- c(&#39;19029&#39;, &#39;19081&#39;, &#39;19082&#39;, &#39;19078&#39;, &#39;19036&#39;, &#39;19033&#39;, 
                     &#39;19070&#39;, &#39;19018&#39;, &#39;19026&#39;, &#39;19064&#39;, &#39;19063&#39;, &#39;19008&#39;,
                     &#39;19073&#39;, &#39;19355&#39;, &#39;19301&#39;, &#39;19050&#39;)

dt %&amp;lt;&amp;gt;% select(ZipCode = &#39;ZIP Code&#39;,
               Enrollees = &#39;Total Number of Consumers Who Have Selected a Marketplace Plan&#39;) %&amp;gt;% 
  mutate(Enrollees = as.integer(Enrollees)) %&amp;gt;%
  filter(ZipCode %in% zips_26thsenate) %&amp;gt;%
  left_join(zipcode, by = c(&#39;ZipCode&#39; = &#39;zip&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summarize the results by zip:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dt %&amp;gt;% 
  select(ZipCode, City = city, Enrollees) %&amp;gt;%
    arrange(desc(Enrollees)) %&amp;gt;% 
    knitr::kable()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
zipmap &amp;lt;- dt %&amp;gt;%
  select(value = Enrollees, region = ZipCode) %&amp;gt;%
  zip_choropleth(zip_zoom = zips_26thsenate, reference_map = TRUE,
                 title = &#39;Healthcare.gov enrollees November 1, 2016 - January 21, 2017&#39;)

plot(zipmap)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/26th_district.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grey’s Anatomy Network of Sexual Relations</title>
      <link>https://gweissman.github.io/post/grey-s-anatomy-network-of-sexual-relations/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/grey-s-anatomy-network-of-sexual-relations/</guid>
      <description>&lt;p&gt;&lt;em&gt;Here&amp;rsquo;s the blog post originally posted on &lt;code&gt;babelgraph.org&lt;/code&gt; on March 25, 2011. The relevant hyperlinks and &lt;code&gt;igraph&lt;/code&gt; code have been updated to reflect the current state of things. The edgelist, despite many more intervening seasons, and thus many more romances, has &lt;strong&gt;not&lt;/strong&gt; been updated. Here is a &lt;a href=&#34;http://badhessian.org/2012/09/lessons-on-exponential-random-graph-modeling-from-greys-anatomy-hook-ups/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nice walkthrough of ERGMs&lt;/a&gt; based on this post &amp;ndash; sorry for the broken link, Ben.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This all began with an introductory presentation about social network analysis to a group of medical students.  What better way to grab their attention than with attractive, fake doctors having sex on television?  Naturally this led to the dense network of sexual contacts between characters on the &lt;a href=&#34;http://en.wikipedia.org/wiki/Grey%27s_Anatomy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grey’s Anatomy television show&lt;/a&gt;.  After viewing many hours of previous episodes and exploring fan pages (especially &lt;a href=&#34;http://insanegrey.livejournal.com/72234.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for an early attempt at a graph representation of sexual contacts), I was able to come up with an extensive but by no means exhaustive list of contacts.  The edge list is available &lt;a href=&#34;https://gweissman.github.io/images/babelgraph/greys/ga_edgelist.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This example uses the &lt;a href=&#34;http://igraph.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;igraph&lt;/a&gt; package for &lt;a href=&#34;http://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt;, both free to download. First we create the graph, give it a layout, and plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(igraph)
ga.data &amp;lt;- read.csv(&#39;ga_edgelist.csv&#39;, header = TRUE)
g &amp;lt;- graph.data.frame(ga.data, directed=FALSE)
summary(g)
g$layout &amp;lt;- layout.fruchterman.reingold(g)
plot(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/greys/plot1.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Without knowing who is represented by each vertex, what can you deduce from the graph? From a public health perspective, if you could test one person for sexually transmitted infections (STIs), who would it be? If you could provide counseling and a free box of condoms to one person, who would it be? If you knew that an epidemic was spreading through this network, who would you want to be to best avoid it?&lt;/p&gt;
&lt;p&gt;Now let’s make the visualization a little more interesting. First we can remove the labels for now, and then change the size of the vertex to represent the degree, or degree centrality, corresponding to the number of partners of each vertex. In the context of transmissible infections, this would indicate the number of people a person could infect or be infected by through sexual contact.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;V(g)$label &amp;lt;- NA # remove labels for now
V(g)$size &amp;lt;- degree(g) * 2 # multiply by 2 for scale
plot(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/greys/plot2.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This tells us about the absolute number of partners, but not much about the relative position within the network. Let’s examine two types of centrality: closeness and betweenness. The closeness centrality is the average shortest path from one vertex to every other on the graph. A high number indicates that a vertex is quickly reachable by the majority of vertices in the graph, while a low number indicates that the vertex is far from most other vertices on the graph. We can calculate the centrality and then rescale the values to create a color scheme to visualize the relative differences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clo &amp;lt;- closeness(g)
# rescale values to match the elements of a color vector
clo.score &amp;lt;- round( (clo - min(clo)) * length(clo) / max(clo) ) + 1
# create color vector, use rev to make red &amp;quot;hot&amp;quot;
clo.colors &amp;lt;- rev(heat.colors(max(clo.score)))
V(g)$color &amp;lt;- clo.colors[ clo.score ]
plot(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/greys/plot3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;It appears there are a few vertices on the red “hot” end of the spectrum, and a few at the “cold” end. Next we do the same for each vertex to calculate the betweenness centrality, which is the number of shortest paths on the network that pass through the vertex. Vertices with high betweenness centrality might be thought of as serving a gatekeeper role in mediating the shortest connections between other vertices.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;btw &amp;lt;- betweenness(g)
btw.score &amp;lt;- round(btw) + 1
btw.colors &amp;lt;- rev(heat.colors(max(btw.score)))
V(g)$color &amp;lt;- btw.colors[ btw.score ]
plot(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/greys/plot4.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This last graph of betweenness indicates slightly more variation among the likely suspects, while the analysis of closeness centrality demonstrated less variation. Why?&lt;/p&gt;
&lt;p&gt;A useful technique in social network analysis is the use of community finding algorithms. Here we use the implementation of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Girvan-Newman algorithm&lt;/a&gt; (&lt;a href=&#34;http://www.pnas.org/content/99/12/7821&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper here&lt;/a&gt;) to detect the underlying community structure of the graph. We will iterate through each merge to determine which cut produces the maximum modularity, and then use that number to calculate the groups.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# this section has been substantially revised to reflect the
# newer version of igraph which does GN membership easily
V(g)$color &amp;lt;- membership(cluster_edge_betweenness(g))
V(g)$size &amp;lt;- 15 # reset to default size
plot(g) # those new default igraph colors are so much nicer, too!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/greys/plot5.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Once you see the graph with names, it is interesting to note the breaks in connectivity around race and age (I guess you have to know the TV characters to appreciate this ;) ) So before seeing the names, back to the original question. Who would you test? Who would you counsel? Who would you vaccinate? Who would you rather be?&lt;/p&gt;
&lt;p&gt;And the winners are…&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;V(g)$color &amp;lt;- &#39;grey&#39;
V(g)$label &amp;lt;- V(g)$name
V(g)$label.cex &amp;lt;- 0.7 # rescale the text size of the label
plot(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/greys/plot6.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The structure of twitter participant relationships in conversations around #Libya, #Bieber, and #Rstats</title>
      <link>https://gweissman.github.io/post/the-structure-of-twitter-participant-relationships-in-conversations-around-libya-bieber-and-rstats/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/the-structure-of-twitter-participant-relationships-in-conversations-around-libya-bieber-and-rstats/</guid>
      <description>&lt;p&gt;&lt;em&gt;Here&amp;rsquo;s the blog post originally posted on &lt;code&gt;babelgraph.org&lt;/code&gt; on April 4, 2011. I suspect the twitter API and much of the code below are &lt;strong&gt;not&lt;/strong&gt; up to date.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I am a recent comer to &lt;a href=&#34;twitter.com&#34;&gt;twitter&lt;/a&gt;, and it took me a few weeks to figure out what this was all about. Who are all these people tweeting each other and what do all these trending hashtags mean? Do these people all know each other? How do they get involved in these conversations? Are people really talking/listening to each other, or just spewing 140 character projectiles out into the void?&lt;/p&gt;
&lt;p&gt;This piqued my interest in the structure of relationships of participants in different twitter conversations. Using &lt;a href=&#34;http://www.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt;, with the &lt;a href=&#34;http://cran.r-project.org/web/packages/twitteR/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitteR&lt;/a&gt; and &lt;a href=&#34;http://igraph.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;igraph&lt;/a&gt; packages, I wondered what I would find&amp;hellip;&lt;/p&gt;
&lt;p&gt;In terms of mapping twitter networks with R, I found &lt;a href=&#34;http://blog.ynada.com/247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; for mapping one’s own personal network of connections. For other visualizations, I found a &lt;a href=&#34;http://flowingdata.com/2008/03/12/17-ways-to-visualize-the-twitter-universe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collection&lt;/a&gt; of various visualizations for the twitterverse, and an &lt;a href=&#34;http://www.orgnet.com/twitter.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;interactive map&lt;/a&gt; of connected users.&lt;/p&gt;
&lt;p&gt;Here I am really interested in the network of participants in a particular conversation. I started off with “#Rstats,” expecting to see a tightly knit community of users who all seem to know each other (at least virtually), sending each other personal updates or tech support tweets. Next I looked at “#Libya,” where I imagined there would be a lot of twitterers with massive followings, but who were unlikely following each other, and producing hordes of news-type updates. Finally I looked at “#Bieber,” (I swear didn’t even know who he was up until a few weeks ago), where I expected to see platoons of small-time twitterers, who likely don’t follow each other, pouring out disconnected digital adulations into the twitterverse.&lt;/p&gt;
&lt;p&gt;The following script was used to construct the networks for each search item.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# an R script to generate the social network of contributors
# to a particular search term
# author: Gary Weissman
# twitter: @garyweissman
 
# choose a conversation and size of search
search.term &amp;lt;- &#39;#Rstats&#39;
search.size &amp;lt;- 50
 
# load required libraries
library(igraph)
library(twitteR)
 
# get recent results from search
search.results &amp;lt;- searchTwitter(search.term,n=search.size)
 
# build the vertex list
vl &amp;lt;- vector()
for (twt in search.results)
  vl &amp;lt;- c(vl,screenName(twt))
 
# calculate contribution of each user in the conversation
vl &amp;lt;- as.data.frame(table(vl))
 
# provide nice names for data
colnames(vl) &amp;lt;- c(&#39;user&#39;,&#39;tweets&#39;)
 
# build the network of relations between contributors
g &amp;lt;- graph.empty(directed=TRUE)
g &amp;lt;- add.vertices(g,nrow(vl),name=as.character(vl$user),
      tweets=vl$tweets)
 
V(g)$followers &amp;lt;- 0 # default to zero
 
# count total number of followers in the larger twitterverse and
# add relationships based on who follows whom within the conversation
for (usr in V(g)) {
 
  # get the user info by name
  tuser &amp;lt;- getUser(V(g)$name[usr+1])
  print(paste(&amp;quot;Getting info on&amp;quot;,screenName(tuser)))
 
  # count total followers in larger twitterverse
  V(g)$followers[usr+1] &amp;lt;- followersCount(tuser)
 
  # access as many followers as we can to see if any
  # appeared in the search results
  followers.list &amp;lt;- userFollowers(tuser,n=1200)
  for (tflwr in followers.list) {
   if (screenName(tflwr) %in% V(g)$name)
    g &amp;lt;- add.edges(g,c(as.vector(V(g)[ name == screenName(tflwr) ]),usr))
  }
  print(&#39;Sleeping 10 min...&#39;)
  Sys.sleep(600); # don&#39;t exceed request limit
}
 
# layout the graph
g$layout &amp;lt;- layout.fruchterman.reingold(g)
# adjust size based on number of total followers
# scale accordingly to keep readable
V(g)$size = log( V(g)$followers ) * 1.8
# set label name and size
V(g)$label=V(g)$name
V(g)$label.cex = 0.6
# do something with color...
tcolors &amp;lt;- rev(heat.colors(max(V(g)$tweets)))
V(g)$color &amp;lt;- tcolors[ V(g)$tweets ]
# make edge arrows less intrusive
E(g)$arrow.size &amp;lt;- 0.3
# make symmetric connections a little easier to read
E(g)$curved &amp;lt;- FALSE # fun to play with
E(g)$color &amp;lt;- &#39;blue&#39;
# now plot...
plot(g)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Technical notes: It wasn’t always clear to me what counted as a “request” in the twitter API, because even a small handful of requests with long results would sometimes seem to count as more toward the hourly quota. Putting a ten minute delay between requests with Sys.sleep(600) was enough to solve the problem. I also tweaked the layouts a bit using tkplot.getcoords(). NB. tkplot() does not permit certain graph attributes such as label.vertex.cex, so you have to remove it before opening the graph in tkplot. You can then just add it back later. More details on this tkplot issue &lt;a href=&#34;https://bugs.launchpad.net/igraph/&amp;#43;bug/572559&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The size of the vertex is proportional to the log of the number of followers in the wider twitterverse, and the color of vertex represents the number of tweets found in the search results by that user. See code above for specifics.&lt;/p&gt;
&lt;p&gt;#Bieber&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/twitter/bieber_plot.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;#Libya&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/twitter/libya_plot.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;#Rstats&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/twitter/rstats_plot.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The networks obviously look very different. Here is a plot of the degree distribution of each graph:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/twitter/abs_deg_dist_by_search_term.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Even more interesting is the simple comparison of the number of edges in each graph. Very telling.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/twitter/vertex_edge_counts_by_search_term.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I did not look at the clustering coefficient here, but that would be interesting, too.&lt;/p&gt;
&lt;p&gt;Does this mean that R users are part of a twitter &lt;em&gt;community&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Limitations on this approach include the size of the user sample (limited by the twitter API to 150 requests per hour), as well as limits on the size of lists retrieved by the &lt;code&gt;userFriends()&lt;/code&gt; method (I believe the limit is 1200 items). These former limits the sample size significantly, and provides only a brief snapshot in time of the participants in any one conversation. The latter favors the construction of edges between twitterers with fewer followers, thus understestimating the total edges that may exist in the network. Is there a workaround for these limitations on the API? I believe requesting whitelist status from twitter, or perhaps authenticating with ROAuth would increase the number of permitted requests, but I haven’t tried them yet.&lt;/p&gt;
&lt;p&gt;Do you know to whom you tweet?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transportation data in R: Why yes, yes you can</title>
      <link>https://gweissman.github.io/post/transportation-data-in-r-why-yes-yes-you-can/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/transportation-data-in-r-why-yes-yes-you-can/</guid>
      <description>&lt;p&gt;Not infrequently I am in a research meeting and someone says, &amp;ldquo;It would be really cool to get data on travel times for people. But I don&amp;rsquo;t know where to find that.&amp;rdquo; Here you will find just that. Please note that Google Maps &lt;a href=&#34;https://developers.google.com/maps/terms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;terms of service&lt;/a&gt; are relevant for large data requests and potential privacy concerns for protected health information.&lt;/p&gt;
&lt;h2 id=&#34;case&#34;&gt;Case&lt;/h2&gt;
&lt;p&gt;Mr. Jones spends all his time at a &lt;a href=&#34;http://www.hobbscoffee.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coffee shop&lt;/a&gt;. Mr. Jones has hypertension, diabetes, and hyperlipidemia, and it doesn&amp;rsquo;t seem likely he will visit his &lt;a href=&#34;https://www.pennmedicine.org/providers/practice/penn-center-for-primary-care&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;primary care doctor&lt;/a&gt; anytime soon. Can we describe his potential travel time between his favorite coffee shop and his primary care doctor to better understand his transportation barriers? Why yes, yes we can.&lt;/p&gt;
&lt;h2 id=&#34;transport&#34;&gt;Transport&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggmap)
# quick start to ggmap: https://github.com/dkahle/ggmap

# Specify where Mr. Jones is and where he&#39;s going
# you can also use specific addresses if you already have them
origin &amp;lt;- &#39;Hobbs Coffee Shop, Swarthmore&#39;
destination &amp;lt;- &#39;Penn Center for Primary Care, Philadelphia&#39;

# get some routes -- driving
my_commute_drive &amp;lt;- route(origin, destination, 
    structure = &#39;route&#39;, mode = &#39;driving&#39;)
print(my_commute_drive)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      m    km     miles seconds   minutes       hours leg       lon      lat
1    30 0.030 0.0186420       8 0.1333333 0.002222222   1 -75.35005 39.90210
2   162 0.162 0.1006668      56 0.9333333 0.015555556   2 -75.35033 39.90226
3  1047 1.047 0.6506058     123 2.0500000 0.034166667   3 -75.35089 39.90116
4  1829 1.829 1.1365406     214 3.5666667 0.059444444   4 -75.35296 39.89203
5   747 0.747 0.4641858      63 1.0500000 0.017500000   5 -75.34469 39.87802
6  4516 4.516 2.8062424     190 3.1666667 0.052777778   6 -75.35102 39.87342
7  5664 5.664 3.5196096     189 3.1500000 0.052500000   7 -75.30839 39.86862
8  5534 5.534 3.4388276     247 4.1166667 0.068611111   8 -75.24609 39.88126
9  2236 2.236 1.3894504     126 2.1000000 0.035000000   9 -75.19435 39.90546
10 1111 1.111 0.6903754      49 0.8166667 0.013611111  10 -75.19280 39.92460
11  347 0.347 0.2156258      20 0.3333333 0.005555556  11 -75.20004 39.93279
12  907 0.907 0.5636098     145 2.4166667 0.040277778  12 -75.19939 39.93587
13 1556 1.556 0.9668984     344 5.7333333 0.095555556  13 -75.19639 39.94367
14  141 0.141 0.0876174      31 0.5166667 0.008611111  14 -75.19798 39.95667
15  114 0.114 0.0708396      30 0.5000000 0.008333333  15 -75.19761 39.95791
16  105 0.105 0.0652470      41 0.6833333 0.011388889  16 -75.19893 39.95809
17   NA    NA        NA      NA        NA          NA  NA -75.19930 39.95843
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or try public transit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# route -- transit
my_commute_transit &amp;lt;- route(origin, destination, 
    structure = &#39;route&#39;, mode = &#39;transit&#39;)
print(my_commute_transit)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      m     km      miles seconds   minutes      hours leg       lon      lat
1    49  0.049  0.0304486      39  0.650000 0.01083333   1 -75.35005 39.90210
2 16750 16.750 10.4084500    1560 26.000000 0.43333333   2 -75.35083 39.90222
3   239  0.239  0.1485146     239  3.983333 0.06638889   3 -75.18166 39.95667
4  1624  1.624  1.0091536     180  3.000000 0.05000000   4 -75.18325 39.95489
5   424  0.424  0.2634736     313  5.216667 0.08694444   5 -75.20197 39.95719
6    NA     NA         NA      NA        NA         NA  NA -75.19930 39.95843
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s see what a walk would look like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# route -- walking
my_commute_walk &amp;lt;- route(origin, destination, 
    structure = &#39;route&#39;, mode = &#39;walking&#39;)
print(my_commute_walk)

# now plot the commute path
qmap(destination, zoom = 11) + 
    geom_path(aes(x=lon, y = lat), color = &#39;red&#39;, 
        size = 1.5, data = my_commute_walk, 
        lineend = &#39;round&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/walk_commute_map.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Mr. Jones, inspired by this map, just put down his triple-shot latte and is walking to his doctor&amp;rsquo;s office right now.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to the blog!</title>
      <link>https://gweissman.github.io/post/welcome-to-the-blog/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/welcome-to-the-blog/</guid>
      <description>&lt;p&gt;Welcome to this blog, which is the new home for thoughts and technical things. As &lt;code&gt;babelgraph.org&lt;/code&gt; has been retired, you can still get the code at:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/gweissman/BabelGraph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BabelGraph Alpha&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over time I will try to migrate the old blog posts (mostly dealing with network analyses and C++ using R) to here along with the very informative user comments, and updates.&lt;/p&gt;
&lt;p&gt;For old time&amp;rsquo;s sake, here is the old thumbnail gallery from the BabelGraph Alpha release (mixed Linux and Mac screenshots). A beautiful sight!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/bg-0-2-linux-directed-tree-small.png&#34; alt=&#34;Directed tree on Linux&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/bg0-2-linux-small.png&#34; alt=&#34;Linux desktop version&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/bg0-2-mac-small.png&#34; alt=&#34;Mac desktop version&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/binary-tree-small.png&#34; alt=&#34;Binary tree&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/heterophily-small.png&#34; alt=&#34;Negative assortativity&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/mutual-homophily-small.png&#34; alt=&#34;Positive assortativity - homophily&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R can write R code, too</title>
      <link>https://gweissman.github.io/post/r-can-write-r-code-too/</link>
      <pubDate>Wed, 15 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/r-can-write-r-code-too/</guid>
      <description>&lt;p&gt;&lt;em&gt;Here&amp;rsquo;s the blog post originally posted on &lt;code&gt;babelgraph.org&lt;/code&gt; on April 14, 2012.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a recent &lt;a href=&#34;http://www.cerebralmastication.com/2012/03/solving-easy-problems-the-hard-way/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/#!/CMastication&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMastication&lt;/a&gt;, a little meme puzzle is presented with the introduction that a preschooler could solve it in 5-10 minutes, a programmer in an hour. I took the bait.&lt;/p&gt;
&lt;p&gt;The original problem goes like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;8809=6
7111=0
2172=0
6666=4
1111=0
3213=0
7662=2
9313=1
0000=4
2222=0
3333=0
5555=0
8193=3
8096=5
7777=0
9999=4
7756=1
6855=3
9881=5
5531=0
2581=? 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;N.B.&lt;/strong&gt; It turns out my strategy is completey wrong, but read on for an experiment with using eval and parse to generate code on the fly. An explanation and computational solution are available at the original site mentioned above.&lt;/p&gt;
&lt;h2 id=&#34;strategy&#34;&gt;Strategy&lt;/h2&gt;
&lt;p&gt;That being said, my first guess was to look for some set of operators &lt;code&gt;+,-,*,/&lt;/code&gt; between each of the digits that would produce the correct response. For example, &lt;code&gt;9313=1&lt;/code&gt; could be calculated as &lt;code&gt;9 / 3 + 1 – 3 = 1&lt;/code&gt;, giving a solution of &lt;code&gt;/+-&lt;/code&gt;. In order to try every combination of the four basic arithmetic operators in three different positions, I had to generate some R code on the fly.&lt;/p&gt;
&lt;h2 id=&#34;r-writes-r&#34;&gt;R writes R&lt;/h2&gt;
&lt;p&gt;One of the things I like about R is that not only can I write R code, but R can write R code, too. We can create a line of code and store it as a variable. Then R will evaluate it whenever we like.&lt;/p&gt;
&lt;p&gt;{% gist gweissman/2377829 %}&lt;/p&gt;
&lt;p&gt;I can create the code dynamically by making a string representation of the code, then parsing it into an expression that can be evaluated as above.&lt;/p&gt;
&lt;p&gt;{% gist /gweissman/2385949 %}&lt;/p&gt;
&lt;p&gt;Clearly it doesn’t give us the right answer.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/babelgraph/4d_teaser_plot.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;But it was fun to code in R with R.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rcpp is smoking fast for agent based models in data frames</title>
      <link>https://gweissman.github.io/post/rcpp-is-smoking-fast-for-agent-based-models-in-data-frames/</link>
      <pubDate>Wed, 15 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/post/rcpp-is-smoking-fast-for-agent-based-models-in-data-frames/</guid>
      <description>&lt;p&gt;&lt;em&gt;Here&amp;rsquo;s the blog post originally posted on &lt;code&gt;babelgraph.org&lt;/code&gt; on July 11, 2012. Thanks to Hadley Wickham for referencing some of content here, and apologies for the broken URL. NB. The original C++ code didn&amp;rsquo;t seem to compile on my computer today. It required a small tweak with &lt;code&gt;NumericVector::create&lt;/code&gt; &amp;ndash; see below.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In particular, R data frames provide a simple framework for representing large cohorts of agents in stochastic epidemiological models, such as those representing disease transmission. This approach is much easier and likely faster than trying to implement cohorts of R objects. In this post we’ll explore a simple agent-based model, and then benchmark a few different approaches to iterating through the cohort. Rcpp outperforms all of them by a few orders of magnitude. Priceless.&lt;/p&gt;
&lt;h2 id=&#34;case&#34;&gt;Case&lt;/h2&gt;
&lt;p&gt;Let’s say we are trying to predict the probability of someone choosing to receive a vaccination in a given year. The decision will be based on their age (&lt;code&gt;age&lt;/code&gt;), gender (&lt;code&gt;female&lt;/code&gt;), and whether or not they were infected with the virus last year (&lt;code&gt;ily&lt;/code&gt;). Let’s make up some data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# construct a cohort
N &amp;lt;- 1000 # cohort size
 
cohort &amp;lt;- data.frame(age=rnorm(N,mean=50,sd=10),
                female=sample(c(0,1),size=N,replace=TRUE),
                ily=sample(c(0,1),size=N,prob=c(0.8,0.2),
                         replace=TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of choosing to be vaccinated will be given by the following function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vaccinate &amp;lt;- function( age, female, ily) {
        # this is based on some pretend regression equation
        p &amp;lt;- 0.25 + 0.3 * 1/(1-exp(0.04 * age)) +  0.1 *ily
        # use some logic
        p &amp;lt;- p * ifelse(female, 1.25 , 0.75)
        # boundary checking
        p &amp;lt;- max(0,p); p &amp;lt;- min(1,p)
        p
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;try-some-iterators-for-loop-apply-ddply&#34;&gt;Try some iterators: for loop, apply, ddply&lt;/h2&gt;
&lt;p&gt;Let’s create a testable function for each strategy. The objective is to take a cohort data frame as input, calculate the vaccination probability for each member of the cohort, then return a data frame with the cohort data plus a new column for the vaccination probability (&lt;code&gt;p&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# The traditional for loop
# Some would say is always a no-no in R...
do_forloop &amp;lt;- function(df) {
     v_prob &amp;lt;- vector(length=nrow(df),mode=&amp;quot;numeric&amp;quot;)
 
     for (x in 1:nrow(df)) {
          v_prob[x] &amp;lt;- vaccinate(df$age[x],
                          df$female[x],df$ily[x])
     }
     data.frame(cbind(df,p=v_prob))
}

# The apply approach
do_apply &amp;lt;- function(df) {
   v_prob &amp;lt;- apply(df, 1, function(x) vaccinate(x[1],x[2],x[3]))
   data.frame(cbind(df,p=v_prob))
}

# ddply approach
library(plyr)
 
do_plyr &amp;lt;- function (df) {
     v_prob &amp;lt;- ddply (df, names(df), function(x)
                vaccinate(x$age,x$female,x$ily))
     data.frame(cbind(df,p=v_prob$V1))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;enter-rcpp&#34;&gt;Enter Rcpp&lt;/h2&gt;
&lt;p&gt;Now rewrite the test using a traditional for-loop in C++ including a helper function to calculate the vaccination probability. I use the inline library so I can embed the C++ directly in the R script, thus obviating additional &lt;code&gt;.cpp&lt;/code&gt; or &lt;code&gt;.h&lt;/code&gt; files. Self-contained code is nice.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create an R function built on C++ code
library(Rcpp)
# required for inline Rcpp calls
library(inline) 
 
# write the C++ code
do_rcpp_src &amp;lt;- &#39;
     // get data from the input data frame
     Rcpp::DataFrame cohort(the_cohort);
     // now extract columns by name from 
     // the data fame into C++ vectors
     std::vector&amp;lt;double&amp;gt; age_v = 
               Rcpp::as&amp;lt; std::vector&amp;lt;double&amp;gt; &amp;gt;(cohort[&amp;quot;age&amp;quot;]);
     std::vector&amp;lt;int&amp;gt; female_v = 
               Rcpp::as&amp;lt; std::vector&amp;lt;int&amp;gt; &amp;gt;(cohort[&amp;quot;female&amp;quot;]);
     std::vector&amp;lt;int&amp;gt; ily_v = 
               Rcpp::as&amp;lt; std::vector&amp;lt;int&amp;gt; &amp;gt;(cohort[&amp;quot;ily&amp;quot;]);
 
     // create a new variable v_prob for export
     std::vector&amp;lt;double&amp;gt; v_prob (ily_v.size());
 
     // iterate over data frame to calculate v_prob
     for (int i = 0; i &amp;lt; v_prob.size() ; i++) {
          v_prob[i] = 
               vaccinate_cxx(age_v[i],female_v[i],ily_v[i]);
     }
 
     // export the old with the new in a combined data frame
     return Rcpp::DataFrame::create( Named(&amp;quot;age&amp;quot;)= age_v, 
                                     Named(&amp;quot;female&amp;quot;) = female_v,
                                     Named(&amp;quot;ily&amp;quot;) = ily_v,
                                     Named(&amp;quot;p&amp;quot;) = v_prob);
&#39;
 
# write the helper function also in C++
# Note small change here from original to include Rcpp:NumericVector::create
# for use with min and max
vaccinate_cxx_src &amp;lt;- &#39;
double vaccinate_cxx (double age, int female, int ily){
        // this is based on some pretend regression equation
        double p = 0.25 + 0.3 * 1/(1-exp(0.004*age)) +  0.1 *ily;
        // use some logic
        p = p * (female ? 1.25 : 0.75);
        // boundary checking
        p = max(Rcpp::NumericVector::create(0,p)); 
        p = min(Rcpp::NumericVector::create(1,p));
        return(p);
}
&#39;
# create an R function to call the C++ code
do_rcpp &amp;lt;- cxxfunction(signature(the_cohort=&amp;quot;data.frame&amp;quot;),
                       do_rcpp_src, plugin=&amp;quot;Rcpp&amp;quot;, 
                       includes=c(&#39;#include &amp;lt;cmath&amp;gt;&#39;,
                                   vaccinate_cxx_src))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;may-the-best-function-win&#34;&gt;May the best function win&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# benchmarking
library(rbenchmark)
 
bm_results &amp;lt;- benchmark(do_forloop(cohort),
           do_apply(cohort),
           do_plyr(cohort),
           do_rcpp(cohort),
           replications=1000)
 
library(lattice)
strategy &amp;lt;- with(bm_results, reorder(test,relative))
barchart(relative ~ strategy, bm_results, 
        ylab=&#39;Relative performance&#39;, 
        xlab=&#39;Strategy&#39;,
        main=&#39;Performance of iteration strategies over data frames in R&#39;, 
        col=&#39;firebrick&#39;,scales=list(x=list(cex=1.2)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://gweissman.github.io/images/looping_speed_compare.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Ree – donc – u – lous.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://gweissman.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Simulated Prospective Evaluation of a Deep Learning Model for Real-Time Prediction of Clinical Deterioration Among Ward Patients</title>
      <link>https://gweissman.github.io/publication/shah_simulated_ccm_2021/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/publication/shah_simulated_ccm_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A vignette-based evaluation of ChatGPT’s ability to provide appropriate and equitable medical advice across care contexts</title>
      <link>https://gweissman.github.io/publication/chatgpt_equitable_2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/publication/chatgpt_equitable_2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Word Embeddings Trained on Published Case Reports Are Lightweight, Effective for Clinical Tasks, and Free of Protected Health Information</title>
      <link>https://gweissman.github.io/publication/embeddings_jbi_2021/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gweissman.github.io/publication/embeddings_jbi_2021/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
